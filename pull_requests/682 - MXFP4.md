## ðŸ”€ [Pull Request #682](https://github.com/ikawrakow/ik_llama.cpp/pull/682) - MXFP4

| **Author** | `ikawrakow` |
| :--- | :--- |
| **State** | ðŸ”€ **Merged** |
| **Source Branch** | `ik/mxfp4` |
| **Target Branch** | `main` |
| **Created** | 2025-08-09 |
| **Updated** | 2025-11-06 |
| **Merged** | 2025-08-09 |

---

## ðŸ“„ Description

This PR adds 4-bit floats as used in OpenAI's gpt-oss models. For compatibility with mainline `llama.cpp`, the implementation follows their somewhat unfortunate approach of using blocks of 32 weights with 8-bit `E8M0` block scales (so 17 bytes per block, thus not having even a 2-byte alignment).

I decided to have this as a separate PR in preparation for adding gpt-oss.

But don't get excited about using `mxfp4` to quantize other models to `fp4`. The zero-bit mantissa in the block scales, along with the `E2M1` choice for the 4-bit floats, results in a horrible quantization accuracy for the 4.25 bpw spent (about the same as `IQ3_K`), unless the model was directly trained with this specific `fp4` variant (as the gpt-oss models). 

Implemented for Zen4, AVX2, ARM_NEON, Metal, CUDA. Vulkan is left for a future PR when I get more serious about the Vulkan back-end.

---

## ðŸ’¬ Conversation

ðŸ‘¤ **espen96** commented on **2025-08-11** at **11:10:53**

the way you describe it, it sounds like there is room for a better / custom variant down the line?

---

ðŸ‘¤ **ikawrakow** commented on **2025-08-11** at **11:24:44**

> the way you describe it, it sounds like there is room for a better / custom variant down the line?

As a bare minimum I would have used blocks of 64 as I'm not aware of any (useful) LLM where tensor dimensions are not a multiple of 64. Alternatively, this being a new quantization type, I would have interleaved 4 or 8 rows, which would have given 4- or 8-byte alignment for a block of 32 weights x 4(8) rows.  I will think about a repacked MXFP4 type after finishing support for the OAI model.

---

ðŸ‘¤ **ubergarm** commented on **2025-11-05** at **22:15:14**

Just for fun did a rough qualitative visualization of a 512x512 8bit grayscale 0-255 bmp image quantized with mxfp4 vs q4_0 vs iq4_kss.

(shown below is an animated gif of a lighthouse with 4 images including the original for comparison)

![quantizations-animated](https://github.com/user-attachments/assets/2abce3ee-9986-4c26-bb96-9d609e4dd2c1)

Some more details on the r/LocaLLaMA post: https://www.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/

---

ðŸ‘¤ **espen96** commented on **2025-11-06** at **13:59:24**

> Just for fun did a rough qualitative visualization of a 512x512 8bit grayscale 0-255 bmp image quantized with mxfp4 vs q4_0 vs iq4_kss.
> 
> (shown below is an animated gif of a lighthouse with 4 images including the original for comparison)
> 
> ![quantizations-animated](https://private-user-images.githubusercontent.com/4724321/510421843-2abce3ee-9986-4c26-bb96-9d609e4dd2c1.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI0MzE2NjYsIm5iZiI6MTc2MjQzMTM2NiwicGF0aCI6Ii80NzI0MzIxLzUxMDQyMTg0My0yYWJjZTNlZS05OTg2LTRjMjYtYmI5Ni05ZDYwOWU0ZGQyYzEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMDZUMTIxNjA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDEzY2FlNmMwYTM0ZmIyZmY5NDdjMDRiMTQ5MzZlNGFmZmNiYmY5ZjFlMTk3Y2JiODVhZTBkNDI1MzZmZGYwNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2Gukh0MleNtIYxwL-PbQfDlnkkqd8M0ggarZrefrsBc) [ ![quantizations-animated](https://private-user-images.githubusercontent.com/4724321/510421843-2abce3ee-9986-4c26-bb96-9d609e4dd2c1.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI0MzE2NjYsIm5iZiI6MTc2MjQzMTM2NiwicGF0aCI6Ii80NzI0MzIxLzUxMDQyMTg0My0yYWJjZTNlZS05OTg2LTRjMjYtYmI5Ni05ZDYwOWU0ZGQyYzEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMDZUMTIxNjA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDEzY2FlNmMwYTM0ZmIyZmY5NDdjMDRiMTQ5MzZlNGFmZmNiYmY5ZjFlMTk3Y2JiODVhZTBkNDI1MzZmZGYwNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2Gukh0MleNtIYxwL-PbQfDlnkkqd8M0ggarZrefrsBc) ](https://private-user-images.githubusercontent.com/4724321/510421843-2abce3ee-9986-4c26-bb96-9d609e4dd2c1.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI0MzE2NjYsIm5iZiI6MTc2MjQzMTM2NiwicGF0aCI6Ii80NzI0MzIxLzUxMDQyMTg0My0yYWJjZTNlZS05OTg2LTRjMjYtYmI5Ni05ZDYwOWU0ZGQyYzEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMDZUMTIxNjA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDEzY2FlNmMwYTM0ZmIyZmY5NDdjMDRiMTQ5MzZlNGFmZmNiYmY5ZjFlMTk3Y2JiODVhZTBkNDI1MzZmZGYwNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2Gukh0MleNtIYxwL-PbQfDlnkkqd8M0ggarZrefrsBc) [ ](https://private-user-images.githubusercontent.com/4724321/510421843-2abce3ee-9986-4c26-bb96-9d609e4dd2c1.gif?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjI0MzE2NjYsIm5iZiI6MTc2MjQzMTM2NiwicGF0aCI6Ii80NzI0MzIxLzUxMDQyMTg0My0yYWJjZTNlZS05OTg2LTRjMjYtYmI5Ni05ZDYwOWU0ZGQyYzEuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMDZUMTIxNjA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDEzY2FlNmMwYTM0ZmIyZmY5NDdjMDRiMTQ5MzZlNGFmZmNiYmY5ZjFlMTk3Y2JiODVhZTBkNDI1MzZmZGYwNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2Gukh0MleNtIYxwL-PbQfDlnkkqd8M0ggarZrefrsBc)
> 
> Some more details on the r/LocaLLaMA post: https://www.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/



mxfp4 terrifies me when visualized like that.

actually, this might be a good way to visualize the typical range of quants. from 8bpw down to Q2 level quants 

the differnece between Q4 quants would be very interesting to see, not to menation the difference between "good old" Q4_K_M and the lower Q5 and Q6 options