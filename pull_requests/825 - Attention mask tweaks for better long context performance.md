## ðŸ”€ [Pull Request #825](https://github.com/ikawrakow/ik_llama.cpp/pull/825) - Attention mask tweaks for better long context performance

| **Author** | `ikawrakow` |
| :--- | :--- |
| **State** | ðŸ”€ **Merged** |
| **Source Branch** | `ik/mask_mt` |
| **Target Branch** | `main` |
| **Created** | 2025-10-12 |
| **Updated** | 2025-10-14 |
| **Merged** | 2025-10-13 |

---

## ðŸ“„ Description

This PR adds the following changes
* The attention mask for batches of 32 or more tokens is computed using multiple threads
* When using flash attention the masks are created directly as `fp16` (instead of first creating an `fp32` mask and then converting it to `fp16` as it is done on the main branch)
* The attention mask padding is reduced to 16 tokens (from 64 on main)

This results in significant PP, and more modest TG performance improvement for long contexts.

Here are some `llama-sweep-bench` results (RTX-4080 in a Ryzen-7950X rig)

## DeepSeek-Lite-16B-Q4_0

```
./bin/llama-sweep-bench -m $model -c 32768 -t 1 -ngl 100 -b 4096 -ub 4096 -fa -mla 3 -fmoe
```

### Main branch

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  4096 |   1024 |      0 |    0.435 |  9422.33 |    4.644 |   220.50 |
|  4096 |   1024 |   4096 |    0.527 |  7773.17 |    4.905 |   208.77 |
|  4096 |   1024 |   8192 |    0.661 |  6195.09 |    5.137 |   199.33 |
|  4096 |   1024 |  12288 |    0.795 |  5150.93 |    5.465 |   187.37 |
|  4096 |   1024 |  16384 |    0.928 |  4413.63 |    5.874 |   174.34 |
|  4096 |   1024 |  20480 |    1.064 |  3849.91 |    6.634 |   154.35 |
|  4096 |   1024 |  24576 |    1.198 |  3420.11 |    6.751 |   151.69 |
|  4096 |   1024 |  28672 |    1.327 |  3086.38 |    6.859 |   149.30 |
|  4096 |   1024 |  32768 |    1.463 |  2799.04 |    7.093 |   144.37 |
|  4096 |   1024 |  36864 |    1.598 |  2562.77 |    7.366 |   139.01 |
|  4096 |   1024 |  40960 |    1.733 |  2363.89 |    7.990 |   128.16 |
|  4096 |   1024 |  45056 |    1.865 |  2196.18 |    8.108 |   126.30 |
|  4096 |   1024 |  49152 |    1.998 |  2049.63 |    8.259 |   123.99 |
|  4096 |   1024 |  53248 |    2.129 |  1924.01 |    8.494 |   120.55 |
|  4096 |   1024 |  57344 |    2.269 |  1805.56 |    8.786 |   116.55 |
|  4096 |   1024 |  61440 |    2.403 |  1704.55 |    9.430 |   108.59 |

### PR

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  4096 |   1024 |      0 |    0.424 |  9654.48 |    4.635 |   220.93 |
|  4096 |   1024 |   4096 |    0.492 |  8321.35 |    4.878 |   209.94 |
|  4096 |   1024 |   8192 |    0.603 |  6790.73 |    5.087 |   201.30 |
|  4096 |   1024 |  12288 |    0.715 |  5724.96 |    5.365 |   190.87 |
|  4096 |   1024 |  16384 |    0.826 |  4955.85 |    5.745 |   178.25 |
|  4096 |   1024 |  20480 |    0.934 |  4384.94 |    6.472 |   158.21 |
|  4096 |   1024 |  24576 |    1.045 |  3918.71 |    6.552 |   156.28 |
|  4096 |   1024 |  28672 |    1.153 |  3553.45 |    6.625 |   154.56 |
|  4096 |   1024 |  32768 |    1.264 |  3240.61 |    6.828 |   149.98 |
|  4096 |   1024 |  36864 |    1.376 |  2976.89 |    7.052 |   145.20 |
|  4096 |   1024 |  40960 |    1.492 |  2744.40 |    7.636 |   134.10 |
|  4096 |   1024 |  45056 |    1.592 |  2572.35 |    7.711 |   132.81 |
|  4096 |   1024 |  49152 |    1.702 |  2406.99 |    7.804 |   131.21 |
|  4096 |   1024 |  53248 |    1.813 |  2259.48 |    7.995 |   128.09 |
|  4096 |   1024 |  57344 |    1.922 |  2130.68 |    8.217 |   124.62 |
|  4096 |   1024 |  61440 |    2.034 |  2014.18 |    8.774 |   116.71 |

I.e., +18% PP and +7.5% TG at 61440 tokens

## Llama3-8B-Instruct-Q4_0

```
./bin/llama-sweep-bench -m $model -c 32768 -t 1 -ngl 100 -ub 2048 -fa 
```

### Main

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  2048 |    512 |      0 |    0.246 |  8339.75 |    4.011 |   127.65 |
|  2048 |    512 |   2048 |    0.269 |  7605.44 |    4.274 |   119.80 |
|  2048 |    512 |   4096 |    0.294 |  6955.22 |    4.481 |   114.26 |
|  2048 |    512 |   6144 |    0.324 |  6317.60 |    4.716 |   108.56 |
|  2048 |    512 |   8192 |    0.348 |  5877.05 |    4.936 |   103.73 |
|  2048 |    512 |  10240 |    0.376 |  5441.01 |    5.170 |    99.04 |
|  2048 |    512 |  12288 |    0.401 |  5105.35 |    5.414 |    94.58 |
|  2048 |    512 |  14336 |    0.430 |  4761.82 |    5.646 |    90.68 |
|  2048 |    512 |  16384 |    0.457 |  4484.98 |    5.850 |    87.52 |
|  2048 |    512 |  18432 |    0.484 |  4233.21 |    6.064 |    84.44 |
|  2048 |    512 |  20480 |    0.510 |  4013.60 |    6.301 |    81.25 |
|  2048 |    512 |  22528 |    0.538 |  3805.00 |    6.524 |    78.48 |
|  2048 |    512 |  24576 |    0.563 |  3638.28 |    6.761 |    75.72 |
|  2048 |    512 |  26624 |    0.592 |  3461.78 |    6.995 |    73.20 |
|  2048 |    512 |  28672 |    0.617 |  3319.94 |    7.193 |    71.18 |
|  2048 |    512 |  30720 |    0.642 |  3189.93 |    7.416 |    69.04 |
|  2048 |    512 |  32768 |    0.670 |  3056.90 |    7.638 |    67.04 |
|  2048 |    512 |  34816 |    0.697 |  2938.81 |    7.884 |    64.94 |
|  2048 |    512 |  36864 |    0.721 |  2838.58 |    8.114 |    63.10 |
|  2048 |    512 |  38912 |    0.749 |  2735.32 |    8.351 |    61.31 |
|  2048 |    512 |  40960 |    0.776 |  2637.84 |    8.556 |    59.84 |
|  2048 |    512 |  43008 |    0.803 |  2551.14 |    8.765 |    58.41 |
|  2048 |    512 |  45056 |    0.827 |  2477.81 |    8.990 |    56.95 |
|  2048 |    512 |  47104 |    0.855 |  2396.09 |    9.225 |    55.50 |
|  2048 |    512 |  49152 |    0.880 |  2326.59 |    9.467 |    54.08 |
|  2048 |    512 |  51200 |    0.908 |  2256.42 |    9.692 |    52.83 |
|  2048 |    512 |  53248 |    0.936 |  2187.44 |    9.937 |    51.52 |
|  2048 |    512 |  55296 |    0.967 |  2118.23 |   10.288 |    49.77 |
|  2048 |    512 |  57344 |    0.987 |  2074.44 |   10.472 |    48.89 |
|  2048 |    512 |  59392 |    1.015 |  2017.72 |   10.601 |    48.30 |
|  2048 |    512 |  61440 |    1.042 |  1965.29 |   10.840 |    47.23 |
|  2048 |    512 |  63488 |    1.073 |  1908.02 |   11.072 |    46.24 |

### PR

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  2048 |    512 |      0 |    0.251 |  8167.82 |    4.010 |   127.69 |
|  2048 |    512 |   2048 |    0.260 |  7882.17 |    4.266 |   120.02 |
|  2048 |    512 |   4096 |    0.279 |  7332.20 |    4.461 |   114.78 |
|  2048 |    512 |   6144 |    0.300 |  6837.26 |    4.680 |   109.40 |
|  2048 |    512 |   8192 |    0.321 |  6372.00 |    4.965 |   103.13 |
|  2048 |    512 |  10240 |    0.345 |  5929.82 |    5.139 |    99.64 |
|  2048 |    512 |  12288 |    0.366 |  5602.65 |    5.392 |    94.95 |
|  2048 |    512 |  14336 |    0.402 |  5096.89 |    5.716 |    89.57 |
|  2048 |    512 |  16384 |    0.404 |  5064.91 |    5.844 |    87.61 |
|  2048 |    512 |  18432 |    0.426 |  4804.24 |    5.997 |    85.37 |
|  2048 |    512 |  20480 |    0.447 |  4579.66 |    6.215 |    82.38 |
|  2048 |    512 |  22528 |    0.469 |  4367.59 |    6.421 |    79.74 |
|  2048 |    512 |  24576 |    0.489 |  4187.03 |    6.659 |    76.89 |
|  2048 |    512 |  26624 |    0.512 |  4001.60 |    6.876 |    74.46 |
|  2048 |    512 |  28672 |    0.532 |  3852.15 |    7.061 |    72.51 |
|  2048 |    512 |  30720 |    0.551 |  3714.14 |    7.277 |    70.35 |
|  2048 |    512 |  32768 |    0.573 |  3576.44 |    7.562 |    67.71 |
|  2048 |    512 |  34816 |    0.592 |  3457.31 |    7.721 |    66.31 |
|  2048 |    512 |  36864 |    0.614 |  3335.89 |    7.946 |    64.43 |
|  2048 |    512 |  38912 |    0.636 |  3221.83 |    8.184 |    62.56 |
|  2048 |    512 |  40960 |    0.656 |  3121.20 |    8.371 |    61.17 |
|  2048 |    512 |  43008 |    0.676 |  3031.47 |    8.568 |    59.76 |
|  2048 |    512 |  45056 |    0.697 |  2938.60 |    8.786 |    58.27 |
|  2048 |    512 |  47104 |    0.718 |  2851.80 |    9.020 |    56.76 |
|  2048 |    512 |  49152 |    0.744 |  2753.27 |    9.240 |    55.41 |
|  2048 |    512 |  51200 |    0.760 |  2696.18 |    9.462 |    54.11 |
|  2048 |    512 |  53248 |    0.781 |  2623.02 |    9.680 |    52.89 |
|  2048 |    512 |  55296 |    0.804 |  2546.88 |    9.868 |    51.89 |
|  2048 |    512 |  57344 |    0.824 |  2486.20 |   10.082 |    50.78 |
|  2048 |    512 |  59392 |    0.846 |  2420.33 |   10.305 |    49.68 |
|  2048 |    512 |  61440 |    0.866 |  2364.07 |   10.519 |    48.68 |
|  2048 |    512 |  63488 |    0.886 |  2311.06 |   10.767 |    47.55 |

I.e., +21% PP and +2.8% TG at 63488 tokens even for a dense model.

## GPT-OSS-20B-MXFP4

This one is interesting, and triggered the investigation that eventually lead to the discovery that the attention mask computation becomes a bottleneck at long contexts. By random coincidence I noticed that the performance dropped significantly as I changed the maximum context length from 30720 tokens to 32768 tokens (32768 is the max. context length I can use on my 16 GiB GPU with u-batch size of 2048 with full GPU offload). The following graph illustrates the issue
   
<img width="792" height="612" alt="u3" src="https://github.com/user-attachments/assets/8737a3ed-fd26-422b-a474-04bc76e9f35d" />

After wasting a lot of time I eventually isolated the time spent in creating the attention masks being the cause for this performance difference (with my guess being that some CPU cache size gets exceeded when the max. context becomes 32k tokens).
  
Here are the sweep-bench tables with this command
```
./bin/llama-sweep-bench -m $model -c 32768 -t 1 -ngl 100 -ub 2048 -fa -fmoe
```

### Main, max. context = 30720

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  2048 |    512 |      0 |    0.218 |  9390.36 |    2.853 |   179.43 |
|  2048 |    512 |   2048 |    0.205 |  9975.50 |    2.944 |   173.93 |
|  2048 |    512 |   4096 |    0.219 |  9364.98 |    2.998 |   170.77 |
|  2048 |    512 |   6144 |    0.235 |  8731.20 |    3.071 |   166.72 |
|  2048 |    512 |   8192 |    0.251 |  8158.55 |    3.139 |   163.12 |
|  2048 |    512 |  10240 |    0.266 |  7702.46 |    3.224 |   158.79 |
|  2048 |    512 |  12288 |    0.281 |  7278.31 |    3.308 |   154.77 |
|  2048 |    512 |  14336 |    0.296 |  6913.87 |    3.361 |   152.31 |
|  2048 |    512 |  16384 |    0.313 |  6546.48 |    3.415 |   149.91 |
|  2048 |    512 |  18432 |    0.329 |  6224.92 |    3.504 |   146.11 |
|  2048 |    512 |  20480 |    0.347 |  5899.48 |    3.573 |   143.30 |
|  2048 |    512 |  22528 |    0.360 |  5681.17 |    3.631 |   140.99 |
|  2048 |    512 |  24576 |    0.375 |  5458.33 |    3.705 |   138.19 |
|  2048 |    512 |  26624 |    0.389 |  5264.55 |    3.778 |   135.52 |
|  2048 |    512 |  28672 |    0.406 |  5042.20 |    3.840 |   133.33 |

### Main, max. context = 32768

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  2048 |    512 |      0 |    0.261 |  7860.24 |    2.845 |   179.98 |
|  2048 |    512 |   2048 |    0.324 |  6330.40 |    2.950 |   173.57 |
|  2048 |    512 |   4096 |    0.395 |  5188.29 |    3.022 |   169.41 |
|  2048 |    512 |   6144 |    0.468 |  4374.38 |    3.114 |   164.40 |
|  2048 |    512 |   8192 |    0.543 |  3773.95 |    3.195 |   160.25 |
|  2048 |    512 |  10240 |    0.615 |  3330.33 |    3.285 |   155.86 |
|  2048 |    512 |  12288 |    0.687 |  2979.50 |    3.371 |   151.90 |
|  2048 |    512 |  14336 |    0.759 |  2697.99 |    3.455 |   148.20 |
|  2048 |    512 |  16384 |    0.833 |  2459.03 |    3.533 |   144.91 |
|  2048 |    512 |  18432 |    0.908 |  2256.52 |    3.619 |   141.48 |
|  2048 |    512 |  20480 |    0.982 |  2085.15 |    3.714 |   137.85 |
|  2048 |    512 |  22528 |    1.054 |  1943.13 |    3.794 |   134.94 |
|  2048 |    512 |  24576 |    1.125 |  1819.79 |    3.886 |   131.74 |
|  2048 |    512 |  26624 |    1.199 |  1708.24 |    3.975 |   128.82 |
|  2048 |    512 |  28672 |    1.274 |  1607.35 |    4.048 |   126.47 |
|  2048 |    512 |  30720 |    1.345 |  1522.13 |    4.137 |   123.78 |

### PR, max. context = 32768

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  2048 |    512 |      0 |    0.194 | 10549.69 |    2.820 |   181.54 |
|  2048 |    512 |   2048 |    0.191 | 10717.74 |    2.887 |   177.36 |
|  2048 |    512 |   4096 |    0.200 | 10238.11 |    2.921 |   175.31 |
|  2048 |    512 |   6144 |    0.209 |  9819.67 |    2.967 |   172.58 |
|  2048 |    512 |   8192 |    0.219 |  9370.81 |    3.010 |   170.11 |
|  2048 |    512 |  10240 |    0.225 |  9106.80 |    3.063 |   167.15 |
|  2048 |    512 |  12288 |    0.235 |  8733.44 |    3.106 |   164.82 |
|  2048 |    512 |  14336 |    0.244 |  8394.47 |    3.154 |   162.32 |
|  2048 |    512 |  16384 |    0.252 |  8140.52 |    3.190 |   160.51 |
|  2048 |    512 |  18432 |    0.262 |  7815.39 |    3.234 |   158.33 |
|  2048 |    512 |  20480 |    0.269 |  7612.87 |    3.286 |   155.83 |
|  2048 |    512 |  22528 |    0.277 |  7381.22 |    3.330 |   153.73 |
|  2048 |    512 |  24576 |    0.288 |  7107.48 |    3.380 |   151.49 |
|  2048 |    512 |  26624 |    0.296 |  6926.57 |    3.424 |   149.52 |
|  2048 |    512 |  28672 |    0.304 |  6742.72 |    3.456 |   148.15 |
|  2048 |    512 |  30720 |    0.310 |  6601.57 |    3.502 |   146.21 |

With the PR there is no performance difference between max. context < 32k or max. context >= 32k. We gain 31% in PP and 10% in TG performance.

---

## ðŸ’¬ Conversation

ðŸ‘¤ **magikRUKKOLA** commented on **2025-10-13** at **23:22:01**

Yeah, it became faster.

sweep-bench for 5bpw quant for Qwen3-Coder before upgrading to latest master:

```
[1760387939] 
[1760387939] main: n_kv_max = 106496, n_batch = 8192, n_ubatch = 8192, flash_attn = 1, n_gpu_layers = 99, n_threads = 64, n_threads_batch = 64
[1760387939] 
[1760387939] |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
[1760387939] |-------|--------|--------|----------|----------|----------|----------|
[1760388246] |  8192 |   2048 |      0 |   25.253 |   324.40 |  254.696 |     8.04 |
[1760388545] |  8192 |   2048 |   8192 |   26.565 |   308.38 |  272.470 |     7.52 |
[1760388864] |  8192 |   2048 |  16384 |   28.410 |   288.35 |  290.123 |     7.06 |
[1760389205] |  8192 |   2048 |  24576 |   31.711 |   258.33 |  309.670 |     6.61 |
[1760389566] |  8192 |   2048 |  32768 |   35.115 |   233.29 |  325.278 |     6.30 |
[1760389947] |  8192 |   2048 |  40960 |   38.937 |   210.39 |  342.695 |     5.98 |
[1760390357] |  8192 |   2048 |  49152 |   41.905 |   195.49 |  367.331 |     5.58 |
[1760390785] |  8192 |   2048 |  57344 |   45.743 |   179.09 |  382.890 |     5.35 |
[1760391235] |  8192 |   2048 |  65536 |   49.615 |   165.11 |  399.854 |     5.12 |
[1760391706] |  8192 |   2048 |  73728 |   52.411 |   156.30 |  418.520 |     4.89 |
```

with the latest master:

```

main: n_kv_max = 106496, n_batch = 8192, n_ubatch = 8192, flash_attn = 1, n_gpu_layers = 99, n_threads = 64, n_threads_batch = 64

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  8192 |   2048 |      0 |   24.774 |   330.67 |  251.420 |     8.15 |
|  8192 |   2048 |   8192 |   26.246 |   312.12 |  268.588 |     7.63 |
|  8192 |   2048 |  16384 |   27.801 |   294.66 |  285.610 |     7.17 |
|  8192 |   2048 |  24576 |   31.010 |   264.17 |  302.888 |     6.76 |
|  8192 |   2048 |  32768 |   34.140 |   239.95 |  319.895 |     6.40 |
|  8192 |   2048 |  40960 |   37.558 |   218.11 |  337.090 |     6.08 |
|  8192 |   2048 |  49152 |   40.588 |   201.83 |  354.161 |     5.78 |
|  8192 |   2048 |  57344 |   44.426 |   184.40 |  371.179 |     5.52 |
|  8192 |   2048 |  65536 |   47.300 |   173.19 |  388.249 |     5.27 |
|  8192 |   2048 |  73728 |   50.679 |   161.65 |  405.241 |     5.05 |
```

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-10-14** at **13:21:56**

It pushed everything up one rung on the sweep bench for GLM-4.6

Before

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.506 |   120.39 |   18.166 |    14.09 |
|  1024 |    256 |   1024 |    8.530 |   120.05 |   18.860 |    13.57 |
|  1024 |    256 |   2048 |    8.649 |   118.39 |   19.395 |    13.20 |
|  1024 |    256 |   3072 |    8.704 |   117.64 |   20.068 |    12.76 |
|  1024 |    256 |   4096 |    8.789 |   116.50 |   20.655 |    12.39 |
|  1024 |    256 |   5120 |    8.896 |   115.10 |   21.268 |    12.04 |
|  1024 |    256 |   6144 |    8.928 |   114.69 |   21.732 |    11.78 |
|  1024 |    256 |   7168 |    8.947 |   114.45 |   22.218 |    11.52 |
|  1024 |    256 |   8192 |    9.060 |   113.02 |   22.751 |    11.25 |





After

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.427 |   121.51 |   18.065 |    14.17 |
|  1024 |    256 |   1024 |    8.474 |   120.84 |   18.292 |    14.00 |
|  1024 |    256 |   2048 |    8.492 |   120.59 |   18.839 |    13.59 |
|  1024 |    256 |   3072 |    8.613 |   118.89 |   19.556 |    13.09 |
|  1024 |    256 |   4096 |    8.666 |   118.16 |   20.119 |    12.72 |
|  1024 |    256 |   5120 |    8.710 |   117.57 |   20.794 |    12.31 |
|  1024 |    256 |   6144 |    8.767 |   116.80 |   21.212 |    12.07 |
|  1024 |    256 |   7168 |    8.869 |   115.46 |   21.688 |    11.80 |
|  1024 |    256 |   8192 |    8.874 |   115.40 |   22.243 |    11.51 |

---

ðŸ‘¤ **ikawrakow** commented on **2025-10-14** at **13:42:57**

Thanks for testing. You are both using very large models where the graph computation is much longer than the mask creation, so performance impact is very modest. The PR gives more significant performance gains when PP > 1000 t/s and/or TG > 100 t/s.