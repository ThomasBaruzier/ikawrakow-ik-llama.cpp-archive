## ðŸ”€ [Pull Request #745](https://github.com/ikawrakow/ik_llama.cpp/pull/745) - Refactor CUDA flash attention

| **Author** | `ikawrakow` |
| :--- | :--- |
| **State** | ðŸ”€ **Merged** |
| **Source Branch** | `ik/cuda_refactor_fattn` |
| **Target Branch** | `main` |
| **Created** | 2025-09-01 |
| **Updated** | 2025-09-02 |
| **Merged** | 2025-09-02 |

---

## ðŸ“„ Description

The main purpose of the PR is to improve CUDA build times as FA compilation times have become unbearable.

But while working on the PR I noticed that not using the FA MMA stream_k implementation for prompt processing is slightly faster on my RTX-4080 GPU (Ada Lovelace arch). About 3-4% faster for attention head size of 64 (e.g., GPT-OSS) at a context of 16k tokens, and about 1-2% for head sizes of 128 and above. Hence, I have removed the unconditional stream_k usage for Ada Lovelace and above. It would be very useful if someone could compare PP performance between the main branch and this PR on Blackwell.

---

## ðŸ’¬ Conversation

ðŸ‘¤ **ikawrakow** commented on **2025-09-02** at **08:12:51**

I guess, if I get feedback it will be via performance regression issues.

---

ðŸ‘¤ **kirnat** commented on **2025-09-02** at **11:03:49**

Thank you very much for all the effort you are putting into this project. Sweep bench runs below were performed with commits 3071 and 3074 on a Blackwell Pro 6000. Let me know if you would like to see any other models or options tested.

## ggml/gpt-oss-120b-mxfp4

**Test Parameters:**
- Command: `llama-sweep-bench -m <model> -ngl 99 -ub 4096 -b 4096 -fa -fmoe -rtr -t 32`
- PP: 4096 tokens
- TG: 1024 tokens

| N_KV | **Commit 3074** | | | **Commit 3071** | | | **Î” Performance** | |
|------|---------|---------|---------|---------|---------|---------|---------|---------|
| | T_PP s | S_PP t/s | S_TG t/s | T_PP s | S_PP t/s | S_TG t/s | PP Î”% | TG Î”% |
| 0 | 0.486 | 8422.54 | 189.69 | 0.504 | 8120.07 | 192.40 | +3.7% | -1.4% |
| 4096 | 0.552 | 7416.84 | 177.69 | 0.555 | 7380.54 | 179.83 | +0.5% | -1.2% |
| 8192 | 0.625 | 6548.77 | 165.24 | 0.630 | 6501.60 | 166.87 | +0.7% | -1.0% |
| 12288 | 0.707 | 5790.91 | 152.74 | 0.712 | 5750.26 | 154.32 | +0.7% | -1.0% |
| 16384 | 0.793 | 5163.03 | 149.15 | 0.784 | 5224.40 | 150.87 | -1.2% | -1.1% |
| 20480 | 0.880 | 4655.59 | 143.33 | 0.867 | 4726.69 | 144.43 | -1.5% | -0.8% |
| 24576 | 0.949 | 4314.26 | 135.27 | 0.947 | 4324.44 | 135.99 | -0.2% | -0.5% |
| 28672 | 1.026 | 3993.72 | 131.52 | 1.015 | 4033.97 | 132.45 | -1.1% | -0.7% |
| 32768 | 1.110 | 3689.62 | 125.67 | 1.093 | 3748.66 | 126.78 | -1.5% | -0.9% |
| 36864 | 1.186 | 3453.96 | 119.13 | 1.189 | 3444.72 | 120.07 | +0.3% | -0.8% |
| 40960 | 1.264 | 3239.95 | 115.48 | 1.247 | 3283.43 | 116.14 | -1.4% | -0.6% |
| 45056 | 1.345 | 3045.90 | 110.91 | 1.355 | 3022.43 | 111.68 | +0.7% | -0.7% |
| 49152 | 1.424 | 2875.66 | 106.03 | 1.446 | 2832.14 | 106.21 | +1.5% | -0.2% |
| 53248 | 1.505 | 2720.97 | 102.72 | 1.533 | 2671.13 | 103.05 | +1.9% | -0.3% |
| 57344 | 1.586 | 2582.56 | 99.25 | 1.626 | 2518.43 | 99.40 | +2.5% | -0.2% |
| 61440 | 1.665 | 2459.48 | 95.10 | 1.685 | 2430.16 | 95.61 | +1.2% | -0.5% |
| 65536 | 1.742 | 2351.93 | 92.77 | 1.773 | 2309.66 | 93.52 | +1.8% | -0.8% |
| 69632 | 1.810 | 2263.07 | 90.44 | 1.854 | 2208.72 | 90.57 | +2.4% | -0.1% |
| 73728 | 1.889 | 2168.89 | 86.63 | 1.951 | 2099.92 | 87.45 | +3.3% | -0.9% |
| 77824 | 1.975 | 2073.52 | 84.60 | 2.040 | 2007.70 | 85.74 | +3.3% | -1.3% |
| 81920 | 2.053 | 1994.95 | 82.55 | 2.163 | 1893.27 | 82.89 | +5.4% | -0.4% |
| 86016 | 2.125 | 1927.24 | 80.12 | 2.213 | 1850.49 | 80.28 | +4.1% | -0.2% |
| 90112 | 2.198 | 1863.29 | 78.38 | 2.309 | 1773.62 | 78.77 | +5.0% | -0.5% |
| 94208 | 2.276 | 1799.79 | 76.53 | 2.398 | 1708.08 | 76.78 | +5.4% | -0.3% |
| 98304 | 2.350 | 1743.10 | 74.12 | 2.552 | 1604.98 | 74.36 | +8.6% | -0.3% |
| 102400 | 2.424 | 1689.60 | 72.45 | 2.636 | 1553.68 | 72.63 | +8.7% | -0.2% |
| 106496 | 2.500 | 1638.49 | 70.44 | 2.663 | 1538.06 | 70.60 | +6.5% | -0.2% |
| 110592 | 2.587 | 1583.51 | 68.31 | 2.820 | 1452.56 | 68.71 | +9.0% | -0.6% |
| 114688 | 2.724 | 1503.67 | 67.08 | 2.893 | 1415.60 | 67.49 | +6.2% | -0.6% |
| 118784 | 2.800 | 1462.90 | 65.76 | 3.040 | 1347.28 | 66.13 | +8.6% | -0.6% |
| 122880 | 2.808 | 1458.91 | 64.16 | 3.126 | 1310.31 | 64.38 | +11.3% | -0.3% |
| 126976 | 2.899 | 1412.67 | 63.06 | 3.193 | 1282.61 | 63.06 | +10.1% | 0.0% |


## unsloth/Meta-Llama-3.1-8B-Instruct-Q8_0
**Test Parameters:**
- Command: `llama-sweep-bench -m <model> -ngl 99 -ub 4096 -b 4096 -fa -rtr -t 8`
- PP: 4096 tokens
- TG: 1024 tokens

| N_KV | **Commit 3074** | | | **Commit 3071** | | | **Î” Performance** | |
|------|---------|---------|---------|---------|---------|---------|---------|---------|
| | T_PP s | S_PP t/s | S_TG t/s | T_PP s | S_PP t/s | S_TG t/s | PP Î”% | TG Î”% |
| 0 | 0.291 | 14085.52 | 155.13 | 0.295 | 13870.97 | 155.91 | +1.4% | -0.5% |
| 4096 | 0.343 | 11942.46 | 145.79 | 0.349 | 11746.55 | 146.47 | +1.7% | -0.5% |
| 8192 | 0.418 | 9805.75 | 136.49 | 0.420 | 9756.17 | 137.24 | +0.5% | -0.5% |
| 12288 | 0.492 | 8318.71 | 126.67 | 0.492 | 8325.56 | 127.21 | 0.0% | -0.4% |
| 16384 | 0.564 | 7259.75 | 122.73 | 0.550 | 7445.78 | 123.21 | -2.5% | -0.4% |
| 20480 | 0.629 | 6512.54 | 117.50 | 0.635 | 6446.56 | 117.88 | +0.9% | -0.3% |
| 24576 | 0.710 | 5767.62 | 111.29 | 0.718 | 5705.27 | 111.67 | +1.1% | -0.3% |
| 28672 | 0.791 | 5180.62 | 108.00 | 0.824 | 4968.11 | 108.44 | +4.2% | -0.4% |
| 32768 | 0.866 | 4731.87 | 103.82 | 0.912 | 4490.87 | 104.18 | +5.3% | -0.3% |
| 36864 | 0.916 | 4471.84 | 99.09 | 1.019 | 4018.49 | 99.44 | +11.3% | -0.4% |
| 40960 | 0.976 | 4195.05 | 96.04 | 1.099 | 3728.54 | 96.54 | +12.5% | -0.5% |
| 45056 | 1.066 | 3840.92 | 92.78 | 1.185 | 3455.21 | 93.24 | +11.2% | -0.5% |
| 49152 | 1.113 | 3679.01 | 88.87 | 1.263 | 3242.00 | 89.20 | +13.5% | -0.4% |
| 53248 | 1.207 | 3394.84 | 86.34 | 1.360 | 3012.83 | 86.48 | +12.7% | -0.2% |
| 57344 | 1.258 | 3256.98 | 83.53 | 1.454 | 2816.61 | 83.73 | +15.6% | -0.2% |
| 61440 | 1.354 | 3024.01 | 80.29 | 1.564 | 2619.60 | 80.67 | +15.5% | -0.5% |
| 65536 | 1.414 | 2895.90 | 78.15 | 1.629 | 2514.87 | 78.60 | +15.2% | -0.6% |
| 69632 | 1.482 | 2763.69 | 75.96 | 1.760 | 2327.06 | 76.19 | +18.8% | -0.3% |
| 73728 | 1.549 | 2644.84 | 73.49 | 1.810 | 2262.67 | 73.43 | +16.9% | +0.1% |
| 77824 | 1.602 | 2556.38 | 71.44 | 1.915 | 2138.62 | 71.59 | +19.5% | -0.2% |
| 81920 | 1.690 | 2423.62 | 69.48 | 2.049 | 1999.19 | 69.68 | +21.2% | -0.3% |
| 86016 | 1.743 | 2350.64 | 67.29 | 2.122 | 1929.88 | 67.44 | +21.8% | -0.2% |
| 90112 | 1.835 | 2232.41 | 65.83 | 2.204 | 1858.77 | 66.01 | +20.1% | -0.3% |
| 94208 | 1.879 | 2180.12 | 64.13 | 2.321 | 1764.40 | 64.34 | +23.5% | -0.3% |
| 98304 | 1.988 | 2060.69 | 62.22 | 2.369 | 1729.21 | 62.53 | +19.2% | -0.5% |
| 102400 | 2.014 | 2033.81 | 60.78 | 2.513 | 1630.18 | 61.09 | +24.8% | -0.5% |
| 106496 | 2.097 | 1953.03 | 59.45 | 2.616 | 1565.50 | 59.55 | +24.8% | -0.2% |
| 110592 | 2.200 | 1861.99 | 57.93 | 2.727 | 1502.14 | 58.09 | +24.0% | -0.3% |
| 114688 | 2.279 | 1797.44 | 56.73 | 2.766 | 1481.08 | 56.86 | +21.4% | -0.2% |
| 118784 | 2.344 | 1747.29 | 55.51 | 2.886 | 1419.02 | 55.65 | +23.2% | -0.3% |
| 122880 | 2.393 | 1711.56 | 54.21 | 3.019 | 1356.84 | 54.42 | +26.2% | -0.4% |
| 126976 | 2.411 | 1698.58 | 53.12 | 3.144 | 1302.83 | 53.31 | +30.4% | -0.4% |

## unsloth/DeepSeek-V3.1-UD-Q2_K_XL
**Test Parameters:**
- Command: `llama-sweep-bench -m <model> -ngl 99 -ot exps=CPU -ub 4096 -b 4096 -fa -fmoe -mla 3 -rtr -t 64 --tb 126`
- PP: 4096 tokens
- TG: 1024 tokens

| N_KV | **Commit 3074** | | | **Commit 3071** | | | **Î” Performance** | |
|------|---------|---------|---------|---------|---------|---------|---------|---------|
| | T_PP s | S_PP t/s | S_TG t/s | T_PP s | S_PP t/s | S_TG t/s | PP Î”% | TG Î”% |
| 0 | 12.937 | 316.60 | 18.89 | 13.085 | 313.03 | 18.67 | +1.1% | +1.2% |
| 4096 | 14.338 | 285.67 | 18.65 | 13.769 | 297.48 | 18.33 | -4.1% | +1.7% |
| 8192 | 14.974 | 273.54 | 18.21 | 15.105 | 271.17 | 17.96 | +0.9% | +1.4% |
| 12288 | 15.307 | 267.59 | 17.75 | 15.404 | 265.91 | 17.60 | +0.6% | +0.9% |
| 16384 | 16.154 | 253.56 | 17.65 | 16.411 | 249.59 | 17.49 | +1.6% | +0.9% |
| 20480 | 17.163 | 238.65 | 17.48 | 17.243 | 237.54 | 17.26 | +0.5% | +1.3% |
| 24576 | 17.580 | 232.99 | 16.95 | 18.180 | 225.30 | 16.79 | +3.4% | +1.0% |

---

ðŸ‘¤ **ikawrakow** commented on **2025-09-02** at **11:56:30**

@kirnat  Thank you for these benchmarks!

So, it looks like this is better overall also on Blackwell.

> Let me know if you would like to see any other models or options tested.

I just created PR [#752](https://github.com/ikawrakow/ik_llama.cpp/issues/752). I would be curious to know how much difference it makes for the `ggml/gpt-oss-120b-mxfp4` model.

---

ðŸ‘¤ **kirnat** commented on **2025-09-02** at **14:05:01**

My pleasure! Prompt processing saw a pretty substantial uplift in the dense model and now also in gpt-oss-120b.

## ggml/gpt-oss-120b-mxfp4
**Test Parameters:**
- Command: `llama-sweep-bench -m <model> -ngl 99 -ub 4096 -b 4096 -fa -fmoe -rtr -t 32`
- PP: 4096 tokens
- TG: 1024 tokens

| N_KV | **PR [#752](https://github.com/ikawrakow/ik_llama.cpp/issues/752)** | | | **Commit 3074** | | | **Î” Performance** | |
|------|---------|---------|---------|---------|---------|---------|---------|---------|
| | T_PP s | S_PP t/s | S_TG t/s | T_PP s | S_PP t/s | S_TG t/s | PP Î”% | TG Î”% |
| 0 | 0.461 | 8875.91 | 191.58 | 0.486 | 8422.54 | 189.69 | +5.4% | +1.0% |
| 4096 | 0.507 | 8084.24 | 176.35 | 0.552 | 7416.84 | 177.69 | +9.0% | -0.8% |
| 8192 | 0.557 | 7348.55 | 162.70 | 0.625 | 6548.77 | 165.24 | +12.2% | -1.5% |
| 12288 | 0.612 | 6691.24 | 150.24 | 0.707 | 5790.91 | 152.74 | +15.5% | -1.6% |
| 16384 | 0.671 | 6102.53 | 145.27 | 0.793 | 5163.03 | 149.15 | +18.2% | -2.6% |
| 20480 | 0.744 | 5508.68 | 139.03 | 0.880 | 4655.59 | 143.33 | +18.3% | -3.0% |
| 24576 | 0.804 | 5096.99 | 132.21 | 0.949 | 4314.26 | 135.27 | +18.1% | -2.3% |
| 28672 | 0.843 | 4856.69 | 127.33 | 1.026 | 3993.72 | 131.52 | +21.6% | -3.2% |
| 32768 | 0.914 | 4482.14 | 121.42 | 1.110 | 3689.62 | 125.67 | +21.5% | -3.4% |
| 36864 | 0.969 | 4229.00 | 116.17 | 1.186 | 3453.96 | 119.13 | +22.4% | -2.5% |
| 40960 | 1.006 | 4069.98 | 111.63 | 1.264 | 3239.95 | 115.48 | +25.6% | -3.3% |
| 45056 | 1.060 | 3865.50 | 107.38 | 1.345 | 3045.90 | 110.91 | +26.9% | -3.2% |
| 49152 | 1.118 | 3663.20 | 102.72 | 1.424 | 2875.66 | 106.03 | +27.4% | -3.1% |
| 53248 | 1.175 | 3487.11 | 99.23 | 1.505 | 2720.97 | 102.72 | +28.2% | -3.4% |
| 57344 | 1.214 | 3372.69 | 95.87 | 1.586 | 2582.56 | 99.25 | +30.6% | -3.4% |
| 61440 | 1.264 | 3239.86 | 92.40 | 1.665 | 2459.48 | 95.10 | +31.7% | -2.8% |
| 65536 | 1.308 | 3131.59 | 89.86 | 1.742 | 2351.93 | 92.77 | +33.1% | -3.1% |
| 69632 | 1.357 | 3018.99 | 86.98 | 1.810 | 2263.07 | 90.44 | +33.4% | -3.8% |
| 73728 | 1.416 | 2892.78 | 84.17 | 1.889 | 2168.89 | 86.63 | +33.4% | -2.8% |
| 77824 | 1.474 | 2779.05 | 82.17 | 1.975 | 2073.52 | 84.60 | +34.0% | -2.9% |
| 81920 | 1.535 | 2669.24 | 79.75 | 2.053 | 1994.95 | 82.55 | +33.8% | -3.4% |
| 86016 | 1.583 | 2586.84 | 77.35 | 2.125 | 1927.24 | 80.12 | +34.2% | -3.5% |
| 90112 | 1.633 | 2508.35 | 75.40 | 2.198 | 1863.29 | 78.38 | +34.6% | -3.8% |
| 94208 | 1.684 | 2432.08 | 73.46 | 2.276 | 1799.79 | 76.53 | +35.1% | -4.0% |
| 98304 | 1.743 | 2349.59 | 71.37 | 2.350 | 1743.10 | 74.12 | +34.8% | -3.7% |
| 102400 | 1.801 | 2274.50 | 69.66 | 2.424 | 1689.60 | 72.45 | +34.6% | -3.9% |
| 106496 | 1.836 | 2230.41 | 67.84 | 2.500 | 1638.49 | 70.44 | +36.1% | -3.7% |
| 110592 | 1.888 | 2169.08 | 66.04 | 2.587 | 1583.51 | 68.31 | +37.0% | -3.3% |
| 114688 | 1.949 | 2101.70 | 64.57 | 2.724 | 1503.67 | 67.08 | +39.8% | -3.7% |
| 118784 | 1.985 | 2063.76 | 63.18 | 2.800 | 1462.90 | 65.76 | +41.1% | -3.9% |
| 122880 | 2.024 | 2023.28 | 61.65 | 2.808 | 1458.91 | 64.16 | +38.7% | -3.9% |
| 126976 | 2.105 | 1945.41 | 60.38 | 2.899 | 1412.67 | 63.06 | +37.7% | -4.2% |

---

ðŸ‘¤ **ikawrakow** commented on **2025-09-02** at **14:18:30**

Thank you!

The PP performance gain is very nice, but the drop in TG performance is unexpected. Are you running on Linux or on Windows?

---

ðŸ‘¤ **kirnat** commented on **2025-09-02** at **14:37:30**

I am running Debian Linux 13 with Nvidia driver 580.76.05 and CUDA 13.0.

Anything else I can check to confirm?

---

ðŸ‘¤ **ikawrakow** commented on **2025-09-02** at **15:03:02**

Thanks. If I understand the why the TG performance gain I'm observing with GPT-OSS-20B, perhaps that will also solve the TG performance regression here.

---

ðŸ‘¤ **kirnat** commented on **2025-09-02** at **15:45:38**

Interesting, I see the same pattern with GPT-OSS-20B, I also tried a few steps without rtr and fmoe but it was consistent with improved PP and the slight TG regression.

Maybe exclusive to Blackwell? If helpful I could possibly try with an RTX 3090 tomorrow in the same system. 

## ggml/gpt-oss-20b-mxfp4
**Test Parameters:**
- Command: `llama-sweep-bench -m <model> -ngl 99 -ub 4096 -b 4096 -fa -fmoe -rtr -t 1`
- PP: 4096 tokens
- TG: 1024 tokens

| N_KV | **PR [#752](https://github.com/ikawrakow/ik_llama.cpp/issues/752)** | | | **Commit 3074** | | | **Î” Performance** | |
|------|---------|---------|---------|---------|---------|---------|---------|---------|
| | T_PP s | S_PP t/s | S_TG t/s | T_PP s | S_PP t/s | S_TG t/s | PP Î”% | TG Î”% |
| 0 | 0.216 | 18962.79 | 277.62 | 0.246 | 16665.51 | 275.40 | +13.8% | +0.8% |
| 4096 | 0.248 | 16547.56 | 254.84 | 0.281 | 14601.77 | 257.00 | +13.3% | -0.8% |
| 8192 | 0.294 | 13921.60 | 235.33 | 0.346 | 11839.90 | 237.80 | +17.6% | -1.0% |
| 12288 | 0.338 | 12125.20 | 217.16 | 0.410 | 9983.38 | 219.33 | +21.5% | -1.0% |
| 16384 | 0.388 | 10567.81 | 209.12 | 0.477 | 8585.51 | 212.93 | +23.1% | -1.8% |
| 20480 | 0.435 | 9413.84 | 200.39 | 0.544 | 7531.31 | 204.30 | +25.0% | -1.9% |
| 24576 | 0.484 | 8457.46 | 190.29 | 0.610 | 6714.06 | 193.11 | +26.0% | -1.5% |
| 28672 | 0.530 | 7723.45 | 181.69 | 0.677 | 6050.10 | 186.91 | +27.7% | -2.8% |
| 32768 | 0.573 | 7147.71 | 173.43 | 0.740 | 5536.19 | 178.12 | +29.1% | -2.6% |
| 36864 | 0.619 | 6612.23 | 164.14 | 0.790 | 5186.86 | 167.63 | +27.5% | -2.1% |
| 40960 | 0.668 | 6133.18 | 156.64 | 0.834 | 4913.21 | 160.52 | +24.8% | -2.4% |
| 45056 | 0.694 | 5905.04 | 149.27 | 0.933 | 4388.20 | 153.47 | +34.6% | -2.7% |
| 49152 | 0.737 | 5558.86 | 141.72 | 0.983 | 4168.32 | 145.36 | +33.4% | -2.5% |
| 53248 | 0.784 | 5227.38 | 136.41 | 1.032 | 3969.02 | 140.12 | +31.7% | -2.6% |
| 57344 | 0.825 | 4963.05 | 131.33 | 1.106 | 3702.23 | 135.19 | +34.1% | -2.9% |
| 61440 | 0.871 | 4705.02 | 126.13 | 1.167 | 3510.08 | 129.56 | +34.0% | -2.6% |
| 65536 | 0.922 | 4444.11 | 122.20 | 1.218 | 3362.99 | 125.81 | +32.1% | -2.9% |
| 69632 | 0.966 | 4239.86 | 118.46 | 1.305 | 3138.39 | 121.84 | +35.1% | -2.8% |
| 73728 | 1.004 | 4080.30 | 114.24 | 1.371 | 2987.23 | 117.19 | +36.6% | -2.5% |
| 77824 | 1.052 | 3892.49 | 110.82 | 1.431 | 2861.87 | 114.35 | +36.0% | -3.1% |
| 81920 | 1.100 | 3724.27 | 107.63 | 1.490 | 2748.47 | 110.56 | +35.5% | -2.7% |
| 86016 | 1.137 | 3603.40 | 104.06 | 1.533 | 2671.79 | 107.21 | +34.9% | -2.9% |
| 90112 | 1.195 | 3428.42 | 100.77 | 1.599 | 2562.16 | 104.97 | +33.8% | -4.0% |
| 94208 | 1.225 | 3342.94 | 98.12 | 1.649 | 2483.18 | 102.22 | +34.6% | -4.0% |
| 98304 | 1.271 | 3221.64 | 95.49 | 1.712 | 2392.71 | 99.09 | +34.6% | -3.6% |
| 102400 | 1.301 | 3147.29 | 92.96 | 1.780 | 2301.46 | 96.69 | +36.8% | -3.9% |
| 106496 | 1.348 | 3039.16 | 90.34 | 1.846 | 2219.21 | 93.63 | +36.9% | -3.5% |
| 110592 | 1.392 | 2941.68 | 87.86 | 1.859 | 2203.80 | 91.17 | +33.5% | -3.6% |
| 114688 | 1.436 | 2852.46 | 86.06 | 1.925 | 2127.97 | 89.39 | +34.0% | -3.7% |
| 118784 | 1.493 | 2743.20 | 83.95 | 1.981 | 2067.16 | 87.29 | +32.7% | -3.8% |
| 122880 | 1.526 | 2683.75 | 81.70 | 2.054 | 1993.69 | 85.03 | +34.6% | -3.9% |
| 126976 | 1.576 | 2599.56 | 79.97 | 2.140 | 1913.59 | 83.12 | +35.8% | -3.8% |