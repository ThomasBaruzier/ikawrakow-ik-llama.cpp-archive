## ðŸ“Œ [Issue #667](https://github.com/ikawrakow/ik_llama.cpp/issues/667) - Bug: llama-quantize does not always follow given custom-q

| **Author** | `erazortt` |
| :--- | :--- |
| **State** | âŒ **Closed** |
| **Created** | 2025-08-01 |
| **Updated** | 2025-08-22 |

---

## ðŸ“„ Description

### What happened?

I gave custom-q to llama-quantize and this is the output I got. Please look for the bold entries for blk.12.attn_output.weight. While the iq4_k matches what I wanted the actual quantization is does at iq6_k:

....
Adding custom rule blk.12.attn_k.weight -> iq4_k
Adding custom rule blk.12.attn_output.weight -> <ins>**_iq4_k_**</ins>
Adding custom rule blk.12.attn_q.weight -> iq4_k
Adding custom rule blk.12.attn_v.weight -> iq6_k
Adding custom rule blk.12.ffn_down.weight -> iq6_k
Adding custom rule blk.12.ffn_gate.weight -> iq4_k
Adding custom rule blk.12.ffn_up.weight -> iq4_k
....
[  92/ 569]               blk.12.ffn_gate.weight - [ 8192, 28672,     1,     1], type =   bf16, Using custom type iq4_k for tensor blk.12.ffn_gate.weight converting to iq4_k .. size =   448.00 MiB ->   126.00 MiB
[  93/ 569]               blk.12.ffn_norm.weight - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  94/ 569]                 blk.12.attn_k.weight - [ 8192,  1024,     1,     1], type =   bf16, Using custom type iq4_k for tensor blk.12.attn_k.weight converting to iq4_k .. size =    16.00 MiB ->     4.50 MiB
[  95/ 569]            blk.12.attn_output.weight - [ 8192,  8192,     1,     1], type =   bf16, Using custom type <ins>**_iq6_k_**</ins> for tensor blk.12.attn_output.weight converting to iq6_k .. size =   128.00 MiB ->    53.00 MiB
[  96/ 569]                 blk.12.attn_q.weight - [ 8192,  8192,     1,     1], type =   bf16, Using custom type iq4_k for tensor blk.12.attn_q.weight converting to iq4_k .. size =   128.00 MiB ->    36.00 MiB
[  97/ 569]                 blk.12.attn_v.weight - [ 8192,  1024,     1,     1], type =   bf16, Using custom type iq6_k for tensor blk.12.attn_v.weight converting to iq6_k .. size =    16.00 MiB ->     6.62 MiB
....
[ 103/ 569]               blk.12.ffn_down.weight - [28672,  8192,     1,     1], type =   bf16, Using custom type iq6_k for tensor blk.12.ffn_down.weight converting to iq6_k .. size =   448.00 MiB ->   185.50 MiB
[ 104/ 569]                 blk.12.ffn_up.weight - [ 8192, 28672,     1,     1], type =   bf16, Using custom type iq4_k for tensor blk.12.ffn_up.weight converting to iq4_k .. size =   448.00 MiB ->   126.00 MiB

### Name and Version

main: build = 1 (bb4c917)
main: built with MSVC 19.44.35213.0 for

### What operating system are you seeing the problem on?

_No response_

### Relevant log output

```shell

```

---

## ðŸ’¬ Conversation

ðŸ‘¤ **ikawrakow** commented on **2025-08-01** at **17:02:00**

Please post all of your custom-q arguments.

---

ðŸ‘¤ **erazortt** commented on **2025-08-04** at **15:51:06**

./llama-quantize --imatrix ../models/nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-imatrix.dat --custom-q output.weight=iq6_k,rope_freqs.weight=f32,token_embd.weight=iq4_k,blk.0.attn_k.weight=iq5_k,blk.0.attn_output.weight=iq4_k,blk.0.attn_q.weight=iq5_k,blk.0.attn_v.weight=iq6_k,blk.0.ffn_down.weight=iq6_k,blk.0.ffn_gate.weight=iq5_k,blk.0.ffn_up.weight=iq5_k,blk.1.attn_k.weight=iq5_k,blk.1.attn_output.weight=iq4_k,blk.1.attn_q.weight=iq5_k,blk.1.attn_v.weight=iq6_k,blk.1.ffn_down.weight=iq6_k,blk.1.ffn_gate.weight=iq5_k,blk.1.ffn_up.weight=iq5_k,blk.2.attn_k.weight=iq4_k,blk.2.attn_output.weight=iq4_k,blk.2.attn_q.weight=iq4_k,blk.2.attn_v.weight=iq6_k,blk.2.ffn_down.weight=iq6_k,blk.2.ffn_gate.weight=iq4_k,blk.2.ffn_up.weight=iq4_k,blk.3.attn_k.weight=iq5_k,blk.3.attn_output.weight=iq4_k,blk.3.attn_q.weight=iq5_k,blk.3.attn_v.weight=iq6_k,blk.3.ffn_down.weight=iq6_k,blk.3.ffn_gate.weight=iq5_k,blk.3.ffn_up.weight=iq5_k,blk.4.attn_k.weight=iq4_ks,blk.4.attn_output.weight=iq4_k,blk.4.attn_q.weight=iq4_ks,blk.4.attn_v.weight=iq6_k,blk.4.ffn_down.weight=iq5_k,blk.4.ffn_gate.weight=iq4_ks,blk.4.ffn_up.weight=iq4_ks,blk.5.attn_k.weight=iq4_ks,blk.5.attn_output.weight=iq4_k,blk.5.attn_q.weight=iq4_ks,blk.5.attn_v.weight=iq6_k,blk.5.ffn_down.weight=iq5_k,blk.5.ffn_gate.weight=iq4_ks,blk.5.ffn_up.weight=iq4_ks,blk.6.ffn_down.weight=iq5_k,blk.6.ffn_gate.weight=iq4_ks,blk.6.ffn_up.weight=iq4_ks,blk.7.ffn_down.weight=iq5_k,blk.7.ffn_gate.weight=iq4_ks,blk.7.ffn_up.weight=iq4_ks,blk.8.attn_k.weight=iq4_ks,blk.8.attn_output.weight=iq4_k,blk.8.attn_q.weight=iq4_ks,blk.8.attn_v.weight=iq6_k,blk.8.ffn_down.weight=iq5_k,blk.8.ffn_gate.weight=iq4_ks,blk.8.ffn_up.weight=iq4_ks,blk.9.attn_k.weight=iq4_k,blk.9.attn_output.weight=iq4_k,blk.9.attn_q.weight=iq4_k,blk.9.attn_v.weight=iq6_k,blk.9.ffn_down.weight=iq6_k,blk.9.ffn_gate.weight=iq4_k,blk.9.ffn_up.weight=iq4_k,blk.10.attn_k.weight=iq4_k,blk.10.attn_output.weight=iq4_k,blk.10.attn_q.weight=iq4_k,blk.10.attn_v.weight=iq6_k,blk.10.ffn_down.weight=iq4_k,blk.10.ffn_gate.weight=iq4_k,blk.10.ffn_up.weight=iq4_k,blk.11.ffn_down.weight=iq4_k,blk.11.ffn_gate.weight=iq4_k,blk.11.ffn_up.weight=iq4_k,blk.12.attn_k.weight=iq4_k,blk.12.attn_output.weight=iq4_k,blk.12.attn_q.weight=iq4_k,blk.12.attn_v.weight=iq6_k,blk.12.ffn_down.weight=iq6_k,blk.12.ffn_gate.weight=iq4_k,blk.12.ffn_up.weight=iq4_k,blk.13.attn_k.weight=iq4_k,blk.13.attn_output.weight=iq4_k,blk.13.attn_q.weight=iq4_k,blk.13.attn_v.weight=iq6_k,blk.13.ffn_down.weight=iq4_k,blk.13.ffn_gate.weight=iq4_k,blk.13.ffn_up.weight=iq4_k,blk.14.attn_k.weight=iq4_k,blk.14.attn_output.weight=iq4_k,blk.14.attn_q.weight=iq4_k,blk.14.attn_v.weight=iq6_k,blk.14.ffn_down.weight=iq4_k,blk.14.ffn_gate.weight=iq4_k,blk.14.ffn_up.weight=iq4_k,blk.15.attn_k.weight=iq4_k,blk.15.attn_output.weight=iq4_k,blk.15.attn_q.weight=iq4_k,blk.15.attn_v.weight=iq6_k,blk.15.ffn_down.weight=iq6_k,blk.15.ffn_gate.weight=iq4_k,blk.15.ffn_up.weight=iq4_k,blk.16.attn_k.weight=iq4_k,blk.16.attn_output.weight=iq4_k,blk.16.attn_q.weight=iq4_k,blk.16.attn_v.weight=iq6_k,blk.16.ffn_down.weight=iq4_k,blk.16.ffn_gate.weight=iq4_k,blk.16.ffn_up.weight=iq4_k,blk.17.attn_k.weight=iq4_k,blk.17.attn_output.weight=iq4_k,blk.17.attn_q.weight=iq4_k,blk.17.attn_v.weight=iq6_k,blk.17.ffn_down.weight=iq4_k,blk.17.ffn_gate.weight=iq4_k,blk.17.ffn_up.weight=iq4_k,blk.18.attn_k.weight=iq4_k,blk.18.attn_output.weight=iq4_k,blk.18.attn_q.weight=iq4_k,blk.18.attn_v.weight=iq6_k,blk.18.ffn_down.weight=iq6_k,blk.18.ffn_gate.weight=iq4_k,blk.18.ffn_up.weight=iq4_k,blk.19.attn_k.weight=iq4_k,blk.19.attn_output.weight=iq4_k,blk.19.attn_q.weight=iq4_k,blk.19.attn_v.weight=iq6_k,blk.19.ffn_down.weight=iq4_k,blk.19.ffn_gate.weight=iq4_k,blk.19.ffn_up.weight=iq4_k,blk.20.attn_k.weight=iq4_k,blk.20.attn_output.weight=iq4_k,blk.20.attn_q.weight=iq4_k,blk.20.attn_v.weight=iq6_k,blk.20.ffn_down.weight=iq4_k,blk.20.ffn_gate.weight=iq4_k,blk.20.ffn_up.weight=iq4_k,blk.21.attn_k.weight=iq4_k,blk.21.attn_output.weight=iq4_k,blk.21.attn_q.weight=iq4_k,blk.21.attn_v.weight=iq6_k,blk.21.ffn_down.weight=iq6_k,blk.21.ffn_gate.weight=iq4_k,blk.21.ffn_up.weight=iq4_k,blk.22.attn_k.weight=iq4_k,blk.22.attn_output.weight=iq4_k,blk.22.attn_q.weight=iq4_k,blk.22.attn_v.weight=iq6_k,blk.22.ffn_down.weight=iq4_k,blk.22.ffn_gate.weight=iq4_k,blk.22.ffn_up.weight=iq4_k,blk.23.attn_k.weight=iq4_k,blk.23.attn_output.weight=iq4_k,blk.23.attn_q.weight=iq4_k,blk.23.attn_v.weight=iq6_k,blk.23.ffn_down.weight=iq4_k,blk.23.ffn_gate.weight=iq4_k,blk.23.ffn_up.weight=iq4_k,blk.24.attn_k.weight=iq4_k,blk.24.attn_output.weight=iq4_k,blk.24.attn_q.weight=iq4_k,blk.24.attn_v.weight=iq6_k,blk.24.ffn_down.weight=iq6_k,blk.24.ffn_gate.weight=iq4_k,blk.24.ffn_up.weight=iq4_k,blk.25.attn_k.weight=iq4_k,blk.25.attn_output.weight=iq4_k,blk.25.attn_q.weight=iq4_k,blk.25.attn_v.weight=iq6_k,blk.25.ffn_down.weight=iq4_k,blk.25.ffn_gate.weight=iq4_k,blk.25.ffn_up.weight=iq4_k,blk.26.attn_k.weight=iq4_k,blk.26.attn_output.weight=iq4_k,blk.26.attn_q.weight=iq4_k,blk.26.attn_v.weight=iq6_k,blk.26.ffn_down.weight=iq4_k,blk.26.ffn_gate.weight=iq4_k,blk.26.ffn_up.weight=iq4_k,blk.27.attn_k.weight=iq4_k,blk.27.attn_output.weight=iq4_k,blk.27.attn_q.weight=iq4_k,blk.27.attn_v.weight=iq6_k,blk.27.ffn_down.weight=iq6_k,blk.27.ffn_gate.weight=iq4_k,blk.27.ffn_up.weight=iq4_k,blk.28.attn_k.weight=iq4_k,blk.28.attn_output.weight=iq4_k,blk.28.attn_q.weight=iq4_k,blk.28.attn_v.weight=iq6_k,blk.28.ffn_down.weight=iq4_k,blk.28.ffn_gate.weight=iq4_k,blk.28.ffn_up.weight=iq4_k,blk.29.attn_k.weight=iq4_k,blk.29.attn_output.weight=iq4_k,blk.29.attn_q.weight=iq4_k,blk.29.attn_v.weight=iq6_k,blk.29.ffn_down.weight=iq4_k,blk.29.ffn_gate.weight=iq4_k,blk.29.ffn_up.weight=iq4_k,blk.30.attn_k.weight=iq4_k,blk.30.attn_output.weight=iq4_k,blk.30.attn_q.weight=iq4_k,blk.30.attn_v.weight=iq6_k,blk.30.ffn_down.weight=iq6_k,blk.30.ffn_gate.weight=iq4_k,blk.30.ffn_up.weight=iq4_k,blk.31.attn_k.weight=iq4_k,blk.31.attn_output.weight=iq4_k,blk.31.attn_q.weight=iq4_k,blk.31.attn_v.weight=iq6_k,blk.31.ffn_down.weight=iq4_k,blk.31.ffn_gate.weight=iq4_k,blk.31.ffn_up.weight=iq4_k,blk.32.attn_k.weight=iq4_k,blk.32.attn_output.weight=iq4_k,blk.32.attn_q.weight=iq4_k,blk.32.attn_v.weight=iq6_k,blk.32.ffn_down.weight=iq4_k,blk.32.ffn_gate.weight=iq4_k,blk.32.ffn_up.weight=iq4_k,blk.33.attn_k.weight=iq4_k,blk.33.attn_output.weight=iq4_k,blk.33.attn_q.weight=iq4_k,blk.33.attn_v.weight=iq6_k,blk.33.ffn_down.weight=iq6_k,blk.33.ffn_gate.weight=iq4_k,blk.33.ffn_up.weight=iq4_k,blk.34.attn_k.weight=iq4_k,blk.34.attn_output.weight=iq4_k,blk.34.attn_q.weight=iq4_k,blk.34.attn_v.weight=iq6_k,blk.34.ffn_down.weight=iq4_k,blk.34.ffn_gate.weight=iq4_k,blk.34.ffn_up.weight=iq4_k,blk.35.attn_k.weight=iq4_k,blk.35.attn_output.weight=iq4_k,blk.35.attn_q.weight=iq4_k,blk.35.attn_v.weight=iq6_k,blk.35.ffn_down.weight=iq4_k,blk.35.ffn_gate.weight=iq4_k,blk.35.ffn_up.weight=iq4_k,blk.36.attn_k.weight=iq4_k,blk.36.attn_output.weight=iq4_k,blk.36.attn_q.weight=iq4_k,blk.36.attn_v.weight=iq6_k,blk.36.ffn_down.weight=iq6_k,blk.36.ffn_gate.weight=iq4_k,blk.36.ffn_up.weight=iq4_k,blk.37.attn_k.weight=iq4_k,blk.37.attn_output.weight=iq4_k,blk.37.attn_q.weight=iq4_k,blk.37.attn_v.weight=iq6_k,blk.37.ffn_down.weight=iq4_k,blk.37.ffn_gate.weight=iq4_k,blk.37.ffn_up.weight=iq4_k,blk.38.attn_k.weight=iq4_k,blk.38.attn_output.weight=iq4_k,blk.38.attn_q.weight=iq4_k,blk.38.attn_v.weight=iq6_k,blk.38.ffn_down.weight=iq4_k,blk.38.ffn_gate.weight=iq4_k,blk.38.ffn_up.weight=iq4_k,blk.39.attn_k.weight=iq4_k,blk.39.attn_output.weight=iq4_k,blk.39.attn_q.weight=iq4_k,blk.39.attn_v.weight=iq6_k,blk.39.ffn_down.weight=iq6_k,blk.39.ffn_gate.weight=iq4_k,blk.39.ffn_up.weight=iq4_k,blk.40.attn_k.weight=iq4_k,blk.40.attn_output.weight=iq4_k,blk.40.attn_q.weight=iq4_k,blk.40.attn_v.weight=iq6_k,blk.40.ffn_down.weight=iq4_k,blk.40.ffn_gate.weight=iq4_k,blk.40.ffn_up.weight=iq4_k,blk.41.attn_k.weight=iq4_k,blk.41.attn_output.weight=iq4_k,blk.41.attn_q.weight=iq4_k,blk.41.attn_v.weight=iq6_k,blk.41.ffn_down.weight=iq4_k,blk.41.ffn_gate.weight=iq4_k,blk.41.ffn_up.weight=iq4_k,blk.42.ffn_down.weight=iq6_k,blk.42.ffn_gate.weight=iq4_k,blk.42.ffn_up.weight=iq4_k,blk.43.ffn_down.weight=iq4_k,blk.43.ffn_gate.weight=iq4_k,blk.43.ffn_up.weight=iq4_k,blk.44.ffn_down.weight=iq4_k,blk.44.ffn_gate.weight=iq4_k,blk.44.ffn_up.weight=iq4_k,blk.45.ffn_down.weight=iq6_k,blk.45.ffn_gate.weight=iq4_k,blk.45.ffn_up.weight=iq4_k,blk.46.ffn_down.weight=iq4_k,blk.46.ffn_gate.weight=iq4_k,blk.46.ffn_up.weight=iq4_k,blk.47.ffn_down.weight=iq4_k,blk.47.ffn_gate.weight=iq4_k,blk.47.ffn_up.weight=iq4_k,blk.48.ffn_down.weight=iq6_k,blk.48.ffn_gate.weight=iq4_k,blk.48.ffn_up.weight=iq4_k,blk.49.ffn_down.weight=iq4_k,blk.49.ffn_gate.weight=iq4_k,blk.49.ffn_up.weight=iq4_k,blk.50.ffn_down.weight=iq4_k,blk.50.ffn_gate.weight=iq4_k,blk.50.ffn_up.weight=iq4_k,blk.51.ffn_down.weight=iq6_k,blk.51.ffn_gate.weight=iq4_k,blk.51.ffn_up.weight=iq4_k,blk.52.attn_k.weight=iq4_k,blk.52.attn_output.weight=iq4_k,blk.52.attn_q.weight=iq4_k,blk.52.attn_v.weight=iq6_k,blk.52.ffn_down.weight=iq4_k,blk.52.ffn_gate.weight=iq4_k,blk.52.ffn_up.weight=iq4_k,blk.53.ffn_down.weight=iq4_k,blk.53.ffn_gate.weight=iq4_k,blk.53.ffn_up.weight=iq4_k,blk.54.ffn_down.weight=iq6_k,blk.54.ffn_gate.weight=iq4_k,blk.54.ffn_up.weight=iq4_k,blk.55.ffn_down.weight=iq4_k,blk.55.ffn_gate.weight=iq4_k,blk.55.ffn_up.weight=iq4_k,blk.56.ffn_down.weight=iq4_k,blk.56.ffn_gate.weight=iq4_k,blk.56.ffn_up.weight=iq4_k,blk.57.ffn_down.weight=iq6_k,blk.57.ffn_gate.weight=iq4_k,blk.57.ffn_up.weight=iq4_k,blk.58.ffn_down.weight=iq4_k,blk.58.ffn_gate.weight=iq4_k,blk.58.ffn_up.weight=iq4_k,blk.59.ffn_down.weight=iq4_k,blk.59.ffn_gate.weight=iq4_k,blk.59.ffn_up.weight=iq4_k,blk.60.ffn_down.weight=iq6_k,blk.60.ffn_gate.weight=iq4_k,blk.60.ffn_up.weight=iq4_k,blk.61.ffn_down.weight=iq4_k,blk.61.ffn_gate.weight=iq4_k,blk.61.ffn_up.weight=iq4_k,blk.62.ffn_down.weight=iq4_k,blk.62.ffn_gate.weight=iq4_k,blk.62.ffn_up.weight=iq4_k,blk.63.ffn_down.weight=iq6_k,blk.63.ffn_gate.weight=iq4_k,blk.63.ffn_up.weight=iq4_k,blk.64.ffn_down.weight=iq4_k,blk.64.ffn_gate.weight=iq4_k,blk.64.ffn_up.weight=iq4_k,blk.65.ffn_down.weight=iq4_k,blk.65.ffn_gate.weight=iq4_k,blk.65.ffn_up.weight=iq4_k,blk.66.ffn_down.weight=iq6_k,blk.66.ffn_gate.weight=iq4_k,blk.66.ffn_up.weight=iq4_k,blk.67.ffn_down.weight=iq4_k,blk.67.ffn_gate.weight=iq4_k,blk.67.ffn_up.weight=iq4_k,blk.68.ffn_down.weight=iq4_k,blk.68.ffn_gate.weight=iq4_k,blk.68.ffn_up.weight=iq4_k,blk.69.ffn_down.weight=iq6_k,blk.69.ffn_gate.weight=iq4_k,blk.69.ffn_up.weight=iq4_k,blk.70.ffn_down.weight=iq6_k,blk.70.ffn_gate.weight=iq4_k,blk.70.ffn_up.weight=iq4_k,blk.71.attn_k.weight=iq4_k,blk.71.attn_output.weight=iq4_k,blk.71.attn_q.weight=iq4_k,blk.71.attn_v.weight=iq6_k,blk.71.ffn_down.weight=iq6_k,blk.71.ffn_gate.weight=iq4_k,blk.71.ffn_up.weight=iq4_k,blk.72.attn_k.weight=iq4_k,blk.72.attn_output.weight=iq4_k,blk.72.attn_q.weight=iq4_k,blk.72.attn_v.weight=iq6_k,blk.72.ffn_down.weight=iq6_k,blk.72.ffn_gate.weight=iq4_k,blk.72.ffn_up.weight=iq4_k,blk.73.attn_k.weight=iq4_k,blk.73.attn_output.weight=iq4_k,blk.73.attn_q.weight=iq4_k,blk.73.attn_v.weight=iq6_k,blk.73.ffn_down.weight=iq6_k,blk.73.ffn_gate.weight=iq4_k,blk.73.ffn_up.weight=iq4_k,blk.74.attn_k.weight=iq4_k,blk.74.attn_output.weight=iq4_k,blk.74.attn_q.weight=iq4_k,blk.74.attn_v.weight=iq6_k,blk.74.ffn_down.weight=iq6_k,blk.74.ffn_gate.weight=iq4_k,blk.74.ffn_up.weight=iq4_k,blk.75.attn_k.weight=iq4_k,blk.75.attn_output.weight=iq4_k,blk.75.attn_q.weight=iq4_k,blk.75.attn_v.weight=iq6_k,blk.75.ffn_down.weight=iq6_k,blk.75.ffn_gate.weight=iq4_k,blk.75.ffn_up.weight=iq4_k,blk.76.attn_k.weight=iq4_k,blk.76.attn_output.weight=iq4_k,blk.76.attn_q.weight=iq4_k,blk.76.attn_v.weight=iq6_k,blk.76.ffn_down.weight=iq6_k,blk.76.ffn_gate.weight=iq4_k,blk.76.ffn_up.weight=iq4_k,blk.77.attn_k.weight=iq4_k,blk.77.attn_output.weight=iq4_k,blk.77.attn_q.weight=iq4_k,blk.77.attn_v.weight=iq6_k,blk.77.ffn_down.weight=iq6_k,blk.77.ffn_gate.weight=iq4_k,blk.77.ffn_up.weight=iq4_k,blk.78.attn_k.weight=iq5_k,blk.78.attn_output.weight=iq4_k,blk.78.attn_q.weight=iq5_k,blk.78.attn_v.weight=iq6_k,blk.78.ffn_down.weight=iq6_k,blk.78.ffn_gate.weight=iq5_k,blk.78.ffn_up.weight=iq5_k,blk.79.attn_k.weight=iq5_k,blk.79.attn_output.weight=iq4_k,blk.79.attn_q.weight=iq5_k,blk.79.attn_v.weight=iq6_k,blk.79.ffn_down.weight=iq6_k,blk.79.ffn_gate.weight=iq5_k,blk.79.ffn_up.weight=iq5_k ../models/Llama-3_3-Nemotron-Super-49B-v1_5-BF16-00001-of-00002.gguf Llama-3_3-Nemotron-Super-49B-v1_5-IQ4_K_L.gguf Q8_0

---

ðŸ‘¤ **ikawrakow** commented on **2025-08-06** at **15:47:32**

The `custom-q` items are of the type `regex=type`. They don't require exact match, so you don't need to write such a long version. The way it works is that the `regex=type` options are put into a list. Then, for each tensor, the list is processed sequentially, checking if `regex` matches the tensor name, and the first matching expression is used.

In your case, you have `output.weight=iq6_k` as the very first item in the list. As `output.weight` matches `attn_output.weight`, `iq6_k` gets used for all `attn_output` tensors. You can fix this by moving `output.weight=iq6_k` to the end of your long list of `custom-q` items.

---

ðŸ‘¤ **ikawrakow** commented on **2025-08-22** at **16:38:06**

User error and no reaction -> closing.