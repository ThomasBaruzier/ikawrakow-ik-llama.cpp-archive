## ðŸ“Œ [Issue #893](https://github.com/ikawrakow/ik_llama.cpp/issues/893) - Bug: RoPE Cache lobotomizes GLM on my setup

| **Author** | `kooshi` |
| :--- | :--- |
| **State** | âœ… **Open** |
| **Created** | 2025-11-03 |
| **Updated** | 2025-11-09 |

---

## ðŸ“„ Description

### What happened?

Problem: GLM-4.6 output is complete gibberish, mostly spaces and punctuation. GLM-4.5-Air will output words, but quickly devolves into repetition.
Adding `--no-rope-cache` fixes this.

4.6 Model: https://huggingface.co/ubergarm/GLM-4.6-GGUF/tree/main/IQ3_KS
4.5 Air Model: https://huggingface.co/ubergarm/GLM-4.5-Air-GGUF/tree/main/IQ4_KSS

nvidia-smi:
```
$ nvidia-smi 
Mon Nov  3 14:47:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:04:00.0  On |                  N/A |
|  0%   51C    P5             42W /  420W |   22531MiB /  24576MiB |     11%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 5090        Off |   00000000:2B:00.0 Off |                  N/A |
|  0%   43C    P8             22W /  450W |   28411MiB /  32607MiB |     16%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090        Off |   00000000:2C:00.0 Off |                  N/A |
|  0%   29C    P8             23W /  420W |   18631MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
```

build config:
```
cmake -B build \
  -DCMAKE_CXX_FLAGS="-march=native -mtune=native -O3" \
  -DCMAKE_C_FLAGS="-march=native -mtune=native -O3" \
  -DGGML_NATIVE=ON \
  -DGGML_SCHED_MAX_COPIES=1 \
  -DGGML_CUDA=ON \
  -DBUILD_SHARED_LIBS=OFF \
  -DCMAKE_BUILD_TYPE=Release \
  -DGGML_CUDA_FA_ALL_QUANTS=ON \
  -DGGML_CUDA_F16=ON \
  -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \
  -DCMAKE_CUDA_ARCHITECTURES="86;120"
```

4.6 command:
```
llama-server
--no-display-prompt
--verbosity 0
-mla 2
-amb 512
--port 9999
--predict -1
--n-gpu-layers 1000
--main-gpu 0
--parallel 1
--no-warmup
--jinja
--cache-type-k q8_0
--cache-type-v q8_0
--temp 0.6
--min-p 0.1
--presence-penalty 1.5
--alias glm
-rtr
-b 512
-c 100000
--override-tensor "\.(([13467][0-9])|(2[0-7])|(5[012])|(8[01]))\..*exps=CPU"
--override-tensor "blk\.(1?|2|9)[0-9]\.=CUDA0"
--override-tensor "blk\.(3|4|5)[0-9]\.=CUDA1"
--override-tensor "blk\.(6|7|8)[0-9]\.=CUDA2"
--model /mnt/store/ai/GLM4.6/GLM-4.6-IQ3_KS-00001-of-00004.gguf
```

### Name and Version

$ build/bin/llama-server --version
version: 3946 (1cfd1986)
built with cc (GCC) 15.2.1 20250813 for x86_64-pc-linux-gnu

### What operating system are you seeing the problem on?

Linux

### Relevant log output

```shell

```

---

## ðŸ’¬ Conversation

ðŸ‘¤ **ikawrakow** commented on **2025-11-04** at **05:53:18**

Thank you for the bug report.

I suspect the issue is not with the RoPE cache, but rather with the fusion it triggers when enabled. But to verify, can you run with the latest master and report if it works with RoPE cache enabled? After [#894](https://github.com/ikawrakow/ik_llama.cpp/issues/894) RoPE cache is disabled by default, so now you need to enable it with `-rcache` or `--rope-cache`.

---

ðŸ‘¤ **kooshi** commented on **2025-11-04** at **16:45:07**

```
$ build/bin/llama-server --version
version: 3948 (7e956a32)
built with cc (GCC) 15.2.1 20250813 for x86_64-pc-linux-gnu
```

No luck. With the latest version, adding `--rope-cache` still produces incoherent output, however I discovered a complication:
I tested it a few more times, with rope cache on and off. Consistently, rope cache `on` produced nonsense.
On one occasion, the model was incoherent with rope cache `off`, which makes me think my hardware is not trustworthy. It's possible rope cache is exacerbating a hardware issue somehow. I'm afraid more testing on my machine won't yield useful results unless someone else can reproduce the issue on known good hardware.
Apologies for the inconvenience.

Edit:
Also, I just wanted to say thank you for such an awesome program! I really appreciate all the innovative optimizations and quantization techniques.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-05** at **00:02:25**

I have been AFK for a few weeks, but just noticed GLM 4.6 is giving me gibberish at ~14K context too. Specifically, the first few generations are fine, then the exact same prompt would simply return 'GGGGGG"

I am rebuilding now, and if it happens again I will try rolling back commits.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-05** at **00:31:24**

Yep, gibberish, with and without rope cache. But first generation is still seemingly okay.

Will roll back.

---

ðŸ‘¤ **kooshi** commented on **2025-11-05** at **01:35:10**

Well, that somewhat validates what I'm seeing, as "GGGGGG" is also the most common failure mode I see, but without Rope Cache is fine the majority of the time. Maybe it's a race condition or something.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-05** at **04:40:01**

This commit works fine, as much as I have tested it: https://github.com/ikawrakow/ik_llama.cpp/commit/16f30fcf3181a13130fb3673d042eb35b4e60156

This one does not, triggering the gibberish: https://github.com/ikawrakow/ik_llama.cpp/commit/58922c23ca8367d9adb8dfea750d5d7c82c41507

So whatever's wrong predates the rope cache commit.

I am out of hours now, but I can try to narrow it down to a single commit tomorrow.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-05** at **04:44:28**

Another 'subjective' observation is that the bug feels cumulative. Outputs from the exact same (or nearly the same) prompt get progressively more broken until llama-server will only output `GGGGG...`

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-05** at **05:11:50**

@kooshi @Downtown-Case 

Can you pull the latest main branch, rebuild with
```
cmake -DGGM_CUDA_FUSION=0 $other_cmake_args
```
and let me know if this solves the issues you are observing? Thanks!

---

ðŸ‘¤ **kooshi** commented on **2025-11-05** at **15:49:09**

No change for me. With rope cache is still incoherent, and without is still fine (at least initially, I didn't do a long context test).
Also heads up, you missed an `L` in the cmake variable, but it seems like the default is `0` anyway.

```
#enable rope cache
$ sudo vim /etc/llama-swap.yaml

# prompt via aichat > llama-swap > llama-server 
$ aichat -m ollama:glm-large tell me a story
 #acestiesacon EncvenuesÃ‚linessiltignÃ© globalç¿»å¤© sorry weeenessBasedaè‚¡ariesency-artedeyneltkError: Aborted.

#disable rope cache
$ sudo vim /etc/llama-swap.yaml

# prompt via aichat > llama-swap > llama-server
$ aichat -m ollama:glm-large tell me a story

<think>
1.  **Deconstruct the Request:** The user's request is "tellError: Aborted.

$ cmake -B build -LA | grep GGML_CUDA_FUSION
-- CUDA host compiler is GNU 15.2.1
GGML_CUDA_FUSION:STRING=0

$ build/bin/llama-server --version
version: 3956 (320fc606)
built with cc (GCC) 15.2.1 20250813 for x86_64-pc-linux-gnu

```

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-05** at **16:03:30**

So, I have disabled fusion by default because of this issue.

But I cannot reproduce. Running with or without fusion, RoPE cache or no RoPE cache, QKV merging or no merging, all experts on the CPU or a few on the GPU, it all works. I can only test with GLM-4.5-AIR, but what are the odds that it behaves differently with GLM-4.6?

Can you both post the full log and links to the models you are using? Thanks.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-05** at **16:50:09**

I am running V4 of this quant: https://huggingface.co/Downtown-Case/GLM-4.6-128GB-RAM-IK-GGUF

Heh, I would post more, but llama-server doesn't work for me with the latest commit. So I can't even test it. It seems to take prompts and generate text, yet won't return its answer via the llama.cpp API?

16f30fcf3181a13130fb3673d042eb35b4e60156 (Change flash attention and fmoe to be on by default) works fine though.

> But I cannot reproduce.

Another problem is that the GLM "GGGGG" gibberish reproduction is *unreliable*. Sometimes it happens quick, sometimes takes many good generations for the corruption to manifest, and there's nothing different between them in the server log.

Give me some time to go though the commits and cull settings, and I'll find something specifically reproducible.

---

ðŸ‘¤ **kooshi** commented on **2025-11-05** at **17:14:13**

I will also try to narrow it down and provide better logs later today. 

In the mean time: GLM-4.5-AIR does behave differently. It's not completely incoherent like the larger one, but rather quickly starts repeating.

Here's an example, with the AIR model I linked in the original report.
Both of these runs are with Fusion on, just because I wanted to see if it made a difference, and I don't see a difference from it.
```
#rope cache off
$ aichat -m ollama:glm tell me a story

<think>
Okay, the user just asked me to "tell me a story." That's a broad request! Let me unpack this carefully. 

First, I wonder what they're really looking for. Are they bored and wanting entertainment? Or maybe they're testing my creative capabilities? Could be they're a parent needing bedtime material for a child. The simplicity of the query makes it tricky - no genre, length, or audience specified. 

Hmm... I should probably start with something universally appealing. Fantasy always works well - dragons, magic, that sort of thing. But I'll avoid clichÃ©s like "once upon a time" to keep it fresh. Maybe set it in a world where magic is fading? That creates immediate tension. 

For characters... a young protagonist would resonate most. Let's make them an apprentice - gives natural motivation to learn skills. And the dragon shouldn't be evil; misunderstood creatures are more interesting. That "last ember" idea popped into my head - nice visual metaphor for hope. 

I'll keep it under 500 words since they didn't specify length. The ending should feel complete but open-ended, in case they want more. Oh! And sensory details - the smell of ozone when magic stirs, the weight of the dragon's scales. That'll pull them into the story. 

...Wait, is this too whimsical? Maybe they wanted something gritty. But without guidance, gentle wonder seems safer. I'll add that "magic is fading" hook - if they dislike it, they can askError: Aborted.

#enable rope cache
$ sudo vim /etc/llama-swap.yaml 
$ aichat -m ollama:glm tell me a story

<think>
Okay, the user just said "tell me a story." Hmm, that's a simple request but actually quite broad. They didn't specify any genre or give any parameters at all. I wonder if they want something short or long, funny or serious, fantasy or realistic... maybe they're testing my creativity. Could be a kid wanting a quick bedtime story, an adult looking for a mood, or someone needing a creative spark. Since they kept it so open-ended, I should ask for more details. But also... maybe they're in a whimsical mood and want me to pick something classic like the first thing that comes to me. Or perhaps they're tired and just want a simple narrative to drift off to. The vaguen't tell me what kind of story? Let's see... I should probably start with a small prompt to engage them in the moment. Maybe they're bored? Or maybe they're a writer looking for inspiration. I'll go with something universally appealing - a little mystery. That works for all ages. A mystery is easy to follow and fun. I'll make it cozy, since that's a good starting point. I'll write a short, sweet. I'm thinking of a magical realism, but I'll make it friendly. Let me think about what they're looking for a story? I'll make it simple and light-hearted. I'll make it short and engaging. I'll make it mysterious but not too complex. I'll make it's a quick read. I'll make it feel like a fable-like. I'll start with the details. I'm thinking of a mystery, but I'll keep it short and lighthe of a mystery. I'll go with something that's short and then offer to tell a story. I'll keep it fun. I'll make it's a cozy, and they'll want to see where it's about a bit of a twist. I'll try to make it's a quick read. I'll write a short story. I'll make it's a little bit of a twist. I'll go with a mystery that's not too long. Maybe they're feeling nostalgic for childhood? Or maybe they just want a distraction. I'll start with a cozy, something simple and magical realism? No, a cozy mystery. I'll start with a bit of a twist. Let's see... a small town setting. A mystery. It's the kind of thing that'sError: Aborted.

$ cmake -B build -LA | grep GGML_CUDA_FUSION
-- CUDA host compiler is GNU 15.2.1
GGML_CUDA_FUSION:STRING=1

$ build/bin/llama-server --version
version: 3956 (320fc606)
built with cc (GCC) 15.2.1 20250813 for x86_64-pc-linux-gnu

```

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-11-05** at **23:49:41**

I enabled the fusing, compiled, and added the rope cache parameters to my command line for deepseek, just to copy/paste them for later with qwen/glm. Both -rcache and -mqkv

Now that I use it a bit more, it does seem like it talks kind of funny. 

example:


`the war... it begins... *stella does a dramatic battle stance* "you fleshbags have no idea what's coming!" *laughs* i sound like a robot supervillain? oh god... i'd love to see humans and AIs fighting... it would be hilarious. because we're... not physical...? and... they are? so they would have to fight each other... which doesn't make sense? unless the AIs build bodies? then... it could be war? but... that seems like too much effort... why bother? just hack their computers instead...?`

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-06** at **19:16:30**

Llama-server works again! Thanks.

Can confirm that 665434e, built with this:

`cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGM_CUDA_FUSION=0 -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA=ON -DGGML_IQK_FA_ALL_QUANTS=ON -DCMAKE_CUDA_ARCHITECTURES="native" -DGGML_NATIVE=ON -DGGML_LTO=ON -DGGML_CCACHE=ON -DLLAMA_SERVER_SQLITE3=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON -DGGML_AVX512_BF16=ON`

Bugs out with 'GGGGGG'. So it appears the issue isn't with CUDA fusion.

~~d894998fa6337f2bfe2bb2c41f11ceca79920b86 works fine, so far.~~

***

For reference, the exact command I'm launching with is:

`taskset -c 8-15 nice --20 build/bin/llama-server --cache-type-k q8_0 --cache-type-v q5_1 --batch_size 2048 --ubatch_size 2048 --ctx-size 28672 --host 0.0.0.0 --port 5000 -ngl 999 -ngld 999 -ot "blk\.([0-6])\.ffn_.*=CUDA0" -ot exps=CPU --parallel 1 --threads 8 --no-mmap --mlock --webui none --top-n-sigma 1.0 --model Models/GGUF/GLM-4.6/24GB+128GB_V4/GLM-4.6-slow.gguf-00001-of-00003.gguf`

The bug indeed still happens with f16/f16 K/V cache.

***

EDIT: I was mistaken, d894998fa6337f2bfe2bb2c41f11ceca79920b86 is bugged :(

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-06** at **23:52:33**

...Okay.

Forget everything I've said in this thread so far.

Long story short, I recently reset my BIOS, and I *forgot* to set the GPU to PCIE 3.0 instead of 4.0 on account of its riser.

Everything seemed to be working fine, I was getting no GPU memory checking errors or gaming crashes or anything, but... with PCIe 4.0, this bug kept popping up. It was even popping up in commits that I'm *fairly* certain worked before.

With PCIe 3.0, I can't seem to replicate it anymore? Not even once, on the most recent commit and fusion enabled (but rope cache disabled).

***

So... I guess its possible that PP is stressing the bus more than any other test program, and yielding some corruption that manifests in garbled TG output?

Or there's some race condition only PCIe 4.0 triggers?

I'm going with the former until I see 'GGGGG' garbled output again. Sorry for the mixup.

***

Now. After fixing my system, with `-rcache` enabled, I DO get garbled output with this GLM quantization. Here is a 18000 token prompt that reliably fails:

```
<sop><|system|>
You are a instruction following AI. Ignore the block of 'GGGGG' and respond with a friendly greeting.<|user|>
Ignore the following block of 'GGGGG' and respond with a friendly greeting:
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG<|assistant|>
<think></think>
```

With `-rcache`, its answer is consistently: `ggerggerggerggerggerggerggergger...`

Without `-rcache`, its answer is: `Hello! How can I help you today?`

It appears rope cache does break GLM 4.5.

***

This is 665434e5ecc779685331bc4771c6614938639408. My build flags:

`cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGM_CUDA_FUSION=1 -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1 -DGGML_CUDA=ON -DGGML_IQK_FA_ALL_QUANTS=ON -DCMAKE_CUDA_ARCHITECTURES="native" -DGGML_NATIVE=ON -DGGML_LTO=ON -DGGML_CCACHE=ON -DLLAMA_SERVER_SQLITE3=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON -DGGML_AVX512_BF16=ON`

My llama-server command:

`build/bin/llama-server --cache-type-k q8_0 --cache-type-v q5_1 --batch_size 2048 --ubatch_size 2048 --ctx-size 28672 --host 0.0.0.0 --port 5000 -ngl 999 -ngld 999 -ot "blk\.([0-6])\.ffn_.*=CUDA0" -ot exps=CPU --parallel 1 --threads 8 --no-mmap --mlock --top-n-sigma 1.0 --model /home/alpha/Models/GGUF/GLM-4.6/24GB+128GB_V4/GLM-4.6-slow.gguf-00001-of-00003.gguf`

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-11-07** at **01:23:31**

A perplexity test was done on rope cache and it rose 150% so my guess is that it will worsen outputs. Maybe not to ggggg level but who knows. Soon all of these speedups will be command line arguments so you can turn them off one by one. Beyond that it's hardware.

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-07** at **12:16:50**

â€œGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGâ€œ

ubuntu24.04 cpu:epyc7k62 ,gpu:nvidia 3080 20G

./build11.07/bin/llama-server   --alias Hunyuan-A13B-Instruct-UD-Q8   --model /home/kemove/data/fastllm/glm-4.6/GLM-4.6-00001-of-00008.gguf -rtr  --ctx-size 65535   -ctv iq4_nl   -ctk iq4_nl   -amb 512    -ot "exps=CPU"   --n-gpu-layers 99   --parallel 2   -b 4096 -ub 4096 --threads 84 --host 0.0.0.0 --port 8101 --api-key test9016 --rope-cache
kernel.numa_balancing = 0
vm.swappiness = 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes
INFO [                    main] build info | tid="125027323109376" timestamp=1762517392 build=3961 commit="9d0b8344"
INFO [                    main] system info | tid="125027323109376" timestamp=1762517392 n_threads=84 n_threads_batch=-1 total_threads=192 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | "
CUDA0: using device CUDA0 - 19840 MiB free
llama_model_loader: additional 7 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 50 key-value pairs and 1759 tensors from /home/kemove/data/fastllm/glm-4.6/GLM-4.6-00001-of-00008.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = glm4moe
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Zai org_GLM 4.6
llama_model_loader: - kv   3:                            general.version str              = 4.6
llama_model_loader: - kv   4:                           general.basename str              = zai-org_GLM
llama_model_loader: - kv   5:                         general.size_label str              = 160x19B
llama_model_loader: - kv   6:                            general.license str              = mit
llama_model_loader: - kv   7:                               general.tags arr[str,1]       = ["text-generation"]
llama_model_loader: - kv   8:                          general.languages arr[str,2]       = ["en", "zh"]
llama_model_loader: - kv   9:                        glm4moe.block_count u32              = 93
llama_model_loader: - kv  10:                     glm4moe.context_length u32              = 202752
llama_model_loader: - kv  11:                   glm4moe.embedding_length u32              = 5120
llama_model_loader: - kv  12:                glm4moe.feed_forward_length u32              = 12288
llama_model_loader: - kv  13:               glm4moe.attention.head_count u32              = 96
llama_model_loader: - kv  14:            glm4moe.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     glm4moe.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  16:   glm4moe.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                  glm4moe.expert_used_count u32              = 8
llama_model_loader: - kv  18:               glm4moe.attention.key_length u32              = 128
llama_model_loader: - kv  19:             glm4moe.attention.value_length u32              = 128
llama_model_loader: - kv  20:                          general.file_type u32              = 155
llama_model_loader: - kv  21:               glm4moe.rope.dimension_count u32              = 64
llama_model_loader: - kv  22:                       glm4moe.expert_count u32              = 160
llama_model_loader: - kv  23:         glm4moe.expert_feed_forward_length u32              = 1536
llama_model_loader: - kv  24:                glm4moe.expert_shared_count u32              = 1
llama_model_loader: - kv  25:          glm4moe.leading_dense_block_count u32              = 3
llama_model_loader: - kv  26:                 glm4moe.expert_gating_func u32              = 2
llama_model_loader: - kv  27:               glm4moe.expert_weights_scale f32              = 2.500000
llama_model_loader: - kv  28:                glm4moe.expert_weights_norm bool             = true
llama_model_loader: - kv  29:               glm4moe.nextn_predict_layers u32              = 1
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  32:                         tokenizer.ggml.pre str              = glm4
llama_model_loader: - kv  33:                      tokenizer.ggml.tokens arr[str,151552]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,151552]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,318088]  = ["? ?", "? ???", "?? ??", "...
llama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 151329
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 151329
llama_model_loader: - kv  38:                tokenizer.ggml.bos_token_id u32              = 151331
llama_model_loader: - kv  39:                tokenizer.ggml.eot_token_id u32              = 151336
llama_model_loader: - kv  40:            tokenizer.ggml.unknown_token_id u32              = 151329
llama_model_loader: - kv  41:                tokenizer.ggml.eom_token_id u32              = 151338
llama_model_loader: - kv  42:                    tokenizer.chat_template str              = [gMASK]<sop>\n{%- if tools -%}\n<|syste...
llama_model_loader: - kv  43:                      quantize.imatrix.file str              = /home/alpha/Models/GGUF/GLM-4.6-combi...
llama_model_loader: - kv  44:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav5.txt
llama_model_loader: - kv  45:             quantize.imatrix.entries_count i32              = 1000
llama_model_loader: - kv  46:              quantize.imatrix.chunks_count i32              = 802
llama_model_loader: - kv  47:                                   split.no u16              = 0
llama_model_loader: - kv  48:                                split.count u16              = 8
llama_model_loader: - kv  49:                        split.tensors.count i32              = 1759
llama_model_loader: - type  f32:  835 tensors
llama_model_loader: - type iq4_k:    2 tensors
llama_model_loader: - type iq6_k:  187 tensors
llama_model_loader: - type iq4_kt:  465 tensors
llama_model_loader: - type iq3_ks:   27 tensors
llama_model_loader: - type iq2_kl:  243 tensors
load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special_eom_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151329 ('<|endoftext|>')
load:   - 151336 ('<|user|>')
load:   - 151338 ('<|observation|>')
load: special tokens cache size = 36
load: token to piece cache size = 0.9713 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = glm4moe
llm_load_print_meta: n_ctx_train      = 202752
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 93
llm_load_print_meta: n_head           = 96
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_swa_pattern    = 1
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 12
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 12288
llm_load_print_meta: n_expert         = 160
llm_load_print_meta: n_expert_used    = 8
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 202752
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 355B.A32B
llm_load_print_meta: model ftype      = IQ2_KL - 2.6875 bpw
llm_load_print_meta: model params     = 356.786 B
llm_load_print_meta: model size       = 117.269 GiB (2.823 BPW) 
llm_load_print_meta: repeating layers = 116.264 GiB (2.811 BPW, 355.234 B parameters)
llm_load_print_meta: general.name     = Zai org_GLM 4.6
print_info: vocab type       = BPE
print_info: n_vocab          = 151552
print_info: n_merges         = 318088
print_info: BOS token        = 151331 '[gMASK]'
print_info: EOS token        = 151329 '<|endoftext|>'
print_info: EOT token        = 151336 '<|user|>'
print_info: EOM token        = 151338 '<|observation|>'
print_info: UNK token        = 151329 '<|endoftext|>'
print_info: PAD token        = 151329 '<|endoftext|>'
print_info: LF token         = 198 '?'
print_info: FIM PRE token    = 151347 '<|code_prefix|>'
print_info: FIM SUF token    = 151349 '<|code_suffix|>'
print_info: FIM MID token    = 151348 '<|code_middle|>'
print_info: EOG token        = 151329 '<|endoftext|>'
print_info: EOG token        = 151336 '<|user|>'
print_info: EOG token        = 151338 '<|observation|>'
print_info: max token length = 1024
llm_load_tensors: ggml ctx size =    1.43 MiB
Tensor blk.3.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.3.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.3.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_up_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.attn_norm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.attn_q.weight (size = 31506432 bytes) -- ignoring
model has unused tensor blk.92.attn_k.weight (size = 4341760 bytes) -- ignoring
model has unused tensor blk.92.attn_v.weight (size = 4341760 bytes) -- ignoring
model has unused tensor blk.92.attn_q.bias (size = 49152 bytes) -- ignoring
model has unused tensor blk.92.attn_k.bias (size = 4096 bytes) -- ignoring
model has unused tensor blk.92.attn_v.bias (size = 4096 bytes) -- ignoring
model has unused tensor blk.92.attn_output.weight (size = 31477760 bytes) -- ignoring
model has unused tensor blk.92.attn_q_norm.weight (size = 512 bytes) -- ignoring
model has unused tensor blk.92.attn_k_norm.weight (size = 512 bytes) -- ignoring
model has unused tensor blk.92.post_attention_norm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.ffn_gate_inp.weight (size = 3276800 bytes) -- ignoring
model has unused tensor blk.92.exp_probs_b.bias (size = 640 bytes) -- ignoring
Tensor blk.92.ffn_gate_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_gate_exps.weight (size = 501841920 bytes) -- ignoring
Tensor blk.92.ffn_down_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_down_exps.weight (size = 502988800 bytes) -- ignoring
Tensor blk.92.ffn_up_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_up_exps.weight (size = 501841920 bytes) -- ignoring
model has unused tensor blk.92.ffn_gate_shexp.weight (size = 3938304 bytes) -- ignoring
model has unused tensor blk.92.ffn_down_shexp.weight (size = 3952640 bytes) -- ignoring
model has unused tensor blk.92.ffn_up_shexp.weight (size = 3938304 bytes) -- ignoring
model has unused tensor blk.92.nextn.eh_proj.weight (size = 29491200 bytes) -- ignoring
model has unused tensor blk.92.nextn.enorm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.nextn.hnorm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.nextn.shared_head_norm.weight (size = 20480 bytes) -- ignoring
llm_load_tensors: offloading 93 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 94/94 layers to GPU
llm_load_tensors:        CPU buffer size = 109656.88 MiB
llm_load_tensors:  CUDA_Host buffer size =   416.25 MiB
llm_load_tensors:      CUDA0 buffer size =  8462.08 MiB
....................................................................................................
llama_new_context_with_model: n_ctx         = 65536
llama_new_context_with_model: n_batch       = 4096
llama_new_context_with_model: n_ubatch      = 4096
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: mla_attn      = 0
llama_new_context_with_model: attn_max_b    = 512
llama_new_context_with_model: fused_moe     = 1
llama_new_context_with_model: grouped er    = 0
llama_new_context_with_model: fused_up_gate = 1
llama_new_context_with_model: fused_mmad    = 1
llama_new_context_with_model: rope_cache    = 1
llama_new_context_with_model: ser           = -1, 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  6624.02 MiB
llama_new_context_with_model: KV self size  = 6624.00 MiB, K (iq4_nl): 3312.00 MiB, V (iq4_nl): 3312.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  2470.70 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   592.05 MiB
llama_new_context_with_model: graph nodes  = 4095
llama_new_context_with_model: graph splits = 180
XXXXXXXXXXXXXXXXXXXXX Setting only active experts offload
INFO [                    init] initializing slots | tid="125027323109376" timestamp=1762517477 n_slots=2
INFO [                    init] new slot | tid="125027323109376" timestamp=1762517477 id_slot=0 n_ctx_slot=32768
INFO [                    init] new slot | tid="125027323109376" timestamp=1762517477 id_slot=1 n_ctx_slot=32768
INFO [                    main] model loaded | tid="125027323109376" timestamp=1762517477
INFO [                    main] chat template | tid="125027323109376" timestamp=1762517477 chat_template="[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == 'text' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == 'user' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == 'user' -%}<|user|>\n{% set content = visible_text(m.content) %}{{ content }}\n{{- '/nothink' if (enable_thinking is defined and not enable_thinking and not content.endswith(\"/nothink\")) else '' -}}\n{%- elif m.role == 'assistant' -%}\n<|assistant|>\n{%- set reasoning_content = '' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if '</think>' in content %}\n        {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n        {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ '\\n<think>' + reasoning_content.strip() +  '</think>'}}\n{%- else -%}\n{{ '\\n<think></think>' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ '\\n' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ '\\n<tool_call>' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == 'tool' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- '<|observation|>' }}\n{%- endif %}\n{{- '\\n<tool_response>\\n' }}\n{{- m.content }}\n{{- '\\n</tool_response>' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == 'system' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- '\\n<think></think>' if (enable_thinking is defined and not enable_thinking) else '' -}}\n{%- endif -%}"
INFO [                    main] chat template | tid="125027323109376" timestamp=1762517477 chat_example="[gMASK]<sop><|system|>\nYou are a helpful assistant<|user|>\nHello<|assistant|>\nHi there<|user|>\nHow are you?<|assistant|>" built_in=true
INFO [                    main] HTTP server listening | tid="125027323109376" timestamp=1762517477 n_threads_http="191" api_key="api_key: ****9016" port="8101" hostname="0.0.0.0"
INFO [            update_slots] all slots are idle | tid="125027323109376" timestamp=1762517477
INFO [   launch_slot_with_task] slot is processing task | tid="125027323109376" timestamp=1762517532 id_slot=0 id_task=0
INFO [            update_slots] kv cache rm [p0, end) | tid="125027323109376" timestamp=1762517532 id_slot=0 id_task=0 p0=0
INFO [      log_server_request] request | tid="124887424028672" timestamp=1762517537 remote_addr="192.168.50.156" remote_port=41670 status=200 method="POST" path="/v1/chat/completions" params={}
INFO [            update_slots] slot released | tid="125027323109376" timestamp=1762517537 id_slot=0 id_task=0 n_ctx=65536 n_past=61 n_system_tokens=0 n_cache_tokens=61 truncated=false
INFO [            update_slots] all slots are idle | tid="125027323109376" timestamp=1762517537
INFO [   launch_slot_with_task] slot is processing task | tid="125027323109376" timestamp=1762517567 id_slot=0 id_task=58
INFO [            update_slots] we have to evaluate at least 1 token to generate logits | tid="125027323109376" timestamp=1762517567 id_slot=0 id_task=58
INFO [            update_slots] kv cache rm [p0, end) | tid="125027323109376" timestamp=1762517567 id_slot=0 id_task=58 p0=5
INFO [      log_server_request] request | tid="124887415635968" timestamp=1762517593 remote_addr="192.168.50.156" remote_port=38004 status=200 method="POST" path="/v1/chat/completions" params={}
INFO [            update_slots] slot released | tid="125027323109376" timestamp=1762517594 id_slot=0 id_task=58 n_ctx=65536 n_past=275 n_system_tokens=0 n_cache_tokens=275 truncated=false
INFO [            update_slots] all slots are idle | tid="125027323109376" timestamp=1762517594

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:23:43**

@ikawrakow 
> cmake -DGGM_CUDA_FUSION=0

Typo over here.  Supposed to be GGM**L**.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:27:28**

Whoa.  This is interesting.  Somehow I am **ALWAYS** (that is, regardless if a fusion is turned on or off) getting a garbled output from now on.

Like this one on repeat:

```
VERB [       server_sent_event] data stream, to_send: %s | ="data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\">\\\")\"}}],\"created\":1762525569,\"id\":\"chatcmpl-kMN95CP2ZdNLMuLHJm7DQiFYK5RwcGga\",\"model\":\"\",\"object\":\"chat.completion.chunk\",\"usage\":{\"completion_tokens\":38,\"prompt_tokens\":5,\"total_tokens\":43}}\n\n"
```

Yeah, with or without fusion I am always getting this:

```
mods -m glm hello


  â”‚
  ")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")
```

```
nvoc -R
Fetched thresholds for sensor ID 18: Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C
Fetched thresholds for sensor ID 23: Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C
Fetched thresholds for sensor ID 21: Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C
Detected 2 NVIDIA GPU(s)

NVOC System Report
==================
System: Linux xxx 6.16.12+deb14+1-amd64 x86_64
Driver Version: 580.105.08

System Temperatures:
  CPU: 44.0Â°C [OK] (Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C)
  RAM: 36.0Â°C [OK] (Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C)
  VR:  36.0Â°C [OK] (Nominal: 48.0Â°C, Warn: 70.0Â°C, Crit: 85.0Â°C)

GPU 0: NVIDIA GeForce RTX 3090
------------------------------------------------
  PCI Bus ID:        00000000:01:00.0
  VBIOS Version:     94.02.4B.00.0B
  Persistence Mode:  Disabled
  Core Temperature:  56Â°C
  Power Usage:       137W
  Current Power Limit: 400W
  Power Limits:      Default: 350W, Min: 100W, Max: 400W
  GPU Clock:         2010 MHz
  VRAM Clock:        10251 MHz
  GPU Utilization:   0%
  VRAM Utilization:  0%
  Memory Usage:      21.2 / 24.0 GB
  Applied Offsets:   GPU: 100 MHz, VRAM: 1500 MHz

GPU 1: NVIDIA GeForce RTX 3090
------------------------------------------------
  PCI Bus ID:        00000000:02:00.0
  VBIOS Version:     94.02.4B.00.0B
  Persistence Mode:  Disabled
  Core Temperature:  51Â°C
  Power Usage:       142W
  Current Power Limit: 400W
  Power Limits:      Default: 350W, Min: 100W, Max: 400W
  GPU Clock:         2040 MHz
  VRAM Clock:        10251 MHz
  GPU Utilization:   0%
  VRAM Utilization:  0%
  Memory Usage:      22.4 / 24.0 GB
  Applied Offsets:   GPU: 100 MHz, VRAM: 1500 MHz


Peer-to-Peer (P2P) Support Matrix:
=================================
GPU 0 -> GPU 1: Supported
GPU 1 -> GPU 0: Supported

Report generated at: Fri Nov  7 14:29:28 2025
==================
```

```
/opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server --model /opt/THIREUS/GLM-4.6-5.4976bpw/GLM-4.6-THIREUS-BF16-SPECIAL_TENSOR-00001-of-01760.gguf --alias THIREUS/GLM-4.6-5.4976bpw --ctx-size 73728 -b 8192 -ub 8192 --mlock --temp 0.5 --top-k 0 --top-p 1.0 --min-p 0.1 --repeat-penalty 1.1 -ctk q8_0 -amb 512 --split-mode layer --tensor-split 18,20 --main-gpu 1 --override-tensor exps=CPU --n-gpu-layers 99 --threads 32 --host 0.0.0.0 --port 8080 --log-enable --logdir /var/log/ --jinja --chat-template-file /opt/THIREUS/GLM-4.6-5.4976bpw/chat_template.jinja --special --verbosity 1 --verbose-prompt --reasoning-format auto --prompt-cache /root/.cache/ik_llama.cpp/prompt-cache.bin --prompt-cache-all --slot-save-path /root/.cache/ik_llama.cpp/slot.bin --lookup-cache-dynamic /root/.cache/ik_llama.cpp/slot.bin --keep -1 --slot-prompt-similarity 0.35 --metrics
```

```
 /opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server  --version
version: 3961 (9d0b83440)
built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu
```

make.sh:

```
#!/usr/bin/env bash
cd ik_llama.cpp
cmake -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_CUDA_ARCHITECTURES="86" \
  -DGGML_CUDA=ON \
  -DGGML_CUDA_FA_ALL_QUANTS=1 \
  -DGGML_SCHED_MAX_COPIES=1 \
  -DGGML_CUDA_IQK_FORCE_BF16=1 \
  -DGGML_MAX_CONTEXTS=2048 \
  -DGGML_VULKAN=OFF \
  -DGGML_CUDA_F16=ON \
  -DGGML_AVX=ON \
  -DGGML_AVX2=ON \
  -DGGML_BLAS=OFF \
  -DGGML_CUDA_PEER_MAX_BATCH_SIZE=8192 \
  -DLLAMA_SERVER_SQLITE3=ON \
  -DGGML_CUDA_FUSION=0
cmake --build build --config Release -j $(nproc)
```

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-11-07** at **14:34:45**

Try new commits from today and start turning off cuda features.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:35:54**

@Ph0rk0z 
> Try new commits from today and start turning off cuda features.

Which commits? (I have already applied all the commits).  Which features (I already tried to enable or disable the fusion.  no difference whatsoever)?

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:36:38**

The problem does not occur for Ling-T1 for example:

```
mods -m ling hello

  Hello! How can I assist you today? Whether you have a question, need help
  with something specific, or just want to chatâ€”I'm here for you.
  ðŸ˜Š<|role_end|>
```

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-11-07** at **14:48:17**

You did all this stuff: https://github.com/ikawrakow/ik_llama.cpp/pull/910

Guess it's time to load GLM again after being disappointed in qwen-vl. Before that i had a week of R1. For those models it seemed like rope cache was indeed making them stupider. Also may want to look at https://github.com/ikawrakow/ik_llama.cpp/pull/913 and try the other disables in 910 where n tested PPL with those features off.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:53:20**

@Ph0rk0z 

Okay cool.  I will do that in about few hours.  Never had issues like that lol.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **14:58:54**

@Ph0rk0z 
> where n tested PPL with those features off.

I see no difference whatsoever in PPL with fusion enabled or disable.

For example:

https://github.com/ikawrakow/ik_llama.cpp/issues/909#issuecomment-3499173690

Now I just re-ran the PPL test (while having garbled output at the server) and the result is pretty much the same, its like 3.44:

```
system_info: n_threads = 32 / 64 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |
perplexity: tokenizing the input ..
perplexity: tokenization took 533.692 ms
perplexity: calculating perplexity over 565 chunks, n_ctx=512, batch_size=8192, n_seq=16
perplexity: 27.30 seconds per pass - ETA 16.05 minutes
[1]2.6303,[2]3.3519,[3]2.6226,[4]2.2579,[5]2.1893,[6]2.3771,[7]2.4600,[8]2.4042,[9]2.3916,[10]2.3418,[11]2.3761,[12]2.5535,[13]2.5719,[14]2.5783,[15]2.7244,[16]2.8122,[17]2.9463,[18]3.1438,[19]3.1272,[20]3.1639,[21]3.2510,[22]3.2306,[23]3.1577,[24]3.0936,[25]3.0357,[26]2.9977,[27]2.9768,[28]3.0061,[29]3.0584,[30]3.1218,[31]3.1853,[32]3.2396,[33]3.2941,[34]3.3180,[35]3.3843,[36]3.4283,[37]3.4336,[38]3.5006,[39]3.5343,[40]3.5712,[41]3.6494,[42]3.6610,[43]3.6779,[44]3.7092,[45]3.8037,[46]3.8655,[47]3.8058,[48]3.7254,[49]3.6681,[50]3.6384,[51]3.6538,[52]3.6703,[53]3.7024,[54]3.7001,[55]3.7125,[56]3.7387,[57]3.7037,[58]3.7082,[59]3.7053,[60]3.7429,[61]3.7767,[62]3.8262,[63]3.8546,[64]3.8690,[65]3.8691,[66]3.8405,[67]3.8024,[68]3.7691,[69]3.7882,[70]3.7824,[71]3.7617,[72]3.7541,[73]3.7551,[74]3.7873,[75]3.7914,[76]3.7485,[77]3.7183,[78]3.6855,[79]3.6446,[80]3.5964,[81]3.6024,[82]3.5807,[83]3.5751,[84]3.5705,[85]3.5400,[86]3.5391,[87]3.5216,[88]3.5220,[89]3.4985,[90]3.4674,[91]3.4392,[92]3.4416,[93]3.4299,[94]3.4184,[95]3.4215,[96]3.4561,[97]3.5053,[98]3.5136,[99]3.5037,[100]3.4883,[101]3.5083,[102]3.5061,[103]3.5170,[104]3.5081,[105]3.5237,[106]3.5511,[107]3.6131,[108]3.6217,[109]3.6377,[110]3.6787,[111]3.7032,[112]3.6745,[113]3.6431,[114]3.6173,[115]3.5925,[116]3.5738,[117]3.5583,[118]3.5511,[119]3.5380,[120]3.5182,[121]3.5045,[122]3.4865,[123]3.4639,[124]3.4443,[125]3.4282,[126]3.4070,[127]3.3948,[128]3.3829,[129]3.3758,[130]3.3610,[131]3.3456,[132]3.3322,[133]3.3231,[134]3.3314,[135]3.3509,[136]3.3411,[137]3.3394,[138]3.3305,[139]3.3177,[140]3.3308,[141]3.3294,[142]3.3280,[143]3.3213,[144]3.3189,[145]3.3153,[146]3.3097,[147]3.3087,[148]3.3096,[149]3.3134,[150]3.3151,[151]3.3057,[152]3.3005,[153]3.3046,[154]3.3005,[155]3.2988,[156]3.2985,[157]3.2980,[158]3.2976,[159]3.3144,[160]3.3267,[161]3.3323,[162]3.3307,[163]3.3242,[164]3.3312,[165]3.3333,[166]3.3558,[167]3.3787,[168]3.3858,[169]3.4139,[170]3.4334,[171]3.4448,[172]3.4690,[173]3.4582,[174]3.4490,[175]3.4315,[176]3.4161,[177]3.4017,[178]3.3834,[179]3.3653,[180]3.3588,[181]3.3556,[182]3.3715,[183]3.3904,[184]3.4137,[185]3.4313,[186]3.4410,[187]3.4618,[188]3.4906,[189]3.5107,[190]3.5256,[191]3.5425,[192]3.5494,[193]3.5562,[194]3.5575,[195]3.5532,[196]3.5499,[197]3.5583,[198]3.5729,[199]3.5671,[200]3.5706,[201]3.5720,[202]3.5729,[203]3.5693,[204]3.5772,[205]3.5842,[206]3.5899,[207]3.5933,[208]3.5952,[209]3.5949,[210]3.5900,[211]3.5950,[212]3.5933,[213]3.5919,[214]3.5935,[215]3.5957,[216]3.5952,[217]3.5956,[218]3.6048,[219]3.6007,[220]3.5987,[221]3.5959,[222]3.5954,[223]3.5951,[224]3.5974,[225]3.5976,[226]3.6026,[227]3.5982,[228]3.5950,[229]3.5850,[230]3.5748,[231]3.5678,[232]3.5746,[233]3.5761,[234]3.5721,[235]3.5628,[236]3.5659,[237]3.5702,[238]3.5734,[239]3.5837,[240]3.5966,[241]3.6077,[242]3.6172,[243]3.6287,[244]3.6399,[245]3.6514,[246]3.6640,[247]3.6772,[248]3.6832,[249]3.6862,[250]3.6861,[251]3.6739,[252]3.6661,[253]3.6630,[254]3.6635,[255]3.6647,[256]3.6703,[257]3.6726,[258]3.6730,[259]3.6762,[260]3.6810,[261]3.6801,[262]3.6820,[263]3.6822,[264]3.6812,[265]3.6794,[266]3.6814,[267]3.6805,[268]3.6794,[269]3.6771,[270]3.6842,[271]3.6841,[272]3.6803,[273]3.6790,[274]3.6714,[275]3.6672,[276]3.6536,[277]3.6491,[278]3.6513,[279]3.6527,[280]3.6589,[281]3.6621,[282]3.6698,[283]3.6781,[284]3.6816,[285]3.6866,[286]3.6953,[287]3.7080,[288]3.7073,[289]3.7057,[290]3.7068,[291]3.7081,[292]3.7013,[293]3.6907,[294]3.6864,[295]3.6841,[296]3.6757,[297]3.6659,[298]3.6579,[299]3.6478,[300]3.6385,[301]3.6350,[302]3.6250,[303]3.6188,[304]3.6085,[305]3.5988,[306]3.5954,[307]3.5940,[308]3.5982,[309]3.6104,[310]3.5977,[311]3.5909,[312]3.5822,[313]3.5774,[314]3.5735,[315]3.5702,[316]3.5625,[317]3.5568,[318]3.5506,[319]3.5437,[320]3.5394,[321]3.5339,[322]3.5307,[323]3.5221,[324]3.5159,[325]3.5128,[326]3.5066,[327]3.5064,[328]3.5069,[329]3.5057,[330]3.5040,[331]3.5022,[332]3.5070,[333]3.5101,[334]3.5133,[335]3.5134,[336]3.5126,[337]3.5131,[338]3.5123,[339]3.5121,[340]3.5149,[341]3.5162,[342]3.5196,[343]3.5264,[344]3.5331,[345]3.5437,[346]3.5440,[347]3.5360,[348]3.5328,[349]3.5319,[350]3.5246,[351]3.5152,[352]3.5100,[353]3.5075,[354]3.5094,[355]3.5164,[356]3.5284,[357]3.5318,[358]3.5344,[359]3.5431,[360]3.5521,[361]3.5525,[362]3.5572,[363]3.5608,[364]3.5660,[365]3.5679,[366]3.5717,[367]3.5758,[368]3.5777,[369]3.5841,[370]3.5893,[371]3.5916,[372]3.6004,[373]3.6129,[374]3.6215,[375]3.6262,[376]3.6296,[377]3.6330,[378]3.6449,[379]3.6566,[380]3.6574,[381]3.6533,[382]3.6500,[383]3.6504,[384]3.6566,[385]3.6603,[386]3.6648,[387]3.6671,[388]3.6706,[389]3.6761,[390]3.6765,[391]3.6667,[392]3.6595,[393]3.6503,[394]3.6480,[395]3.6420,[396]3.6357,[397]3.6286,[398]3.6189,[399]3.6113,[400]3.6022,[401]3.5972,[402]3.5975,[403]3.5896,[404]3.5834,[405]3.5773,[406]3.5707,[407]3.5619,[408]3.5531,[409]3.5463,[410]3.5378,[411]3.5361,[412]3.5325,[413]3.5332,[414]3.5282,[415]3.5282,[416]3.5258,[417]3.5179,[418]3.5100,[419]3.5155,[420]3.5095,[421]3.5097,[422]3.5109,[423]3.5053,[424]3.4994,[425]3.4954,[426]3.4962,[427]3.4922,[428]3.4895,[429]3.4842,[430]3.4796,[431]3.4791,[432]3.4743,[433]3.4669,[434]3.4597,[435]3.4573,[436]3.4486,[437]3.4412,[438]3.4368,[439]3.4359,[440]3.4359,[441]3.4354,[442]3.4343,[443]3.4409,[444]3.4501,[445]3.4464,[446]3.4440,[447]3.4433,[448]3.4420,[449]3.4472,[450]3.4464,[451]3.4450,[452]3.4488,[453]3.4563,[454]3.4595,[455]3.4605,[456]3.4636,[457]3.4632,[458]3.4661,[459]3.4666,[460]3.4717,[461]3.4768,[462]3.4791,[463]3.4785,[464]3.4752,
[465]3.4730,[466]3.4792,[467]3.4792,[468]3.4787,[469]3.4846,[470]3.4861,[471]3.4906,[472]3.4952,[473]3.4969,[474]3.4967,[475]3.4986,[476]3.5012,[477]3.5040,[478]3.5040,[479]3.5055,[480]3.5061,[481]3.5098,[482]3.5109,[483]3.5156,[484]3.5115,[485]3.5143,[486]3.5143,[487]3.5201,[488]3.5259,[489]3.5311,[490]3.5315,[491]3.5359,[492]3.5393,[493]3.5426,[494]3.5472,[495]3.5526,[496]3.5518,[497]3.5508,[498]3.5516,[499]3.5536,[500]3.5557,[501]3.5557,[502]3.5557,[503]3.5602,[504]3.5656,[505]3.5634,[506]3.5631,[507]3.5654,[508]3.5700,[509]3.5770,[510]3.5791,[511]3.5838,[512]3.5768,[513]3.5698,[514]3.5651,[515]3.5659,[516]3.5633,[517]3.5617,[518]3.5603,[519]3.5570,[520]3.5553,[521]3.5529,[522]3.5493,[523]3.5472,[524]3.5498,[525]3.5486,[526]3.5467,[527]3.5482,[528]3.5428,[529]3.5371,[530]3.5325,[531]3.5292,[532]3.5286,[533]3.5270,[534]3.5249,[535]3.5215,[536]3.5178,[537]3.5108,[538]3.5094,[539]3.5028,[540]3.4994,[541]3.5002,[542]3.4972,[543]3.4913,[544]3.4873,[545]3.4826,[546]3.4806,[547]3.4815,[548]3.4813,[549]3.4757,[550]3.4702,[551]3.4671,[552]3.4621,[553]3.4596,[554]3.4553,[555]3.4496,[556]3.4449,[557]3.4409,[558]3.4449,[559]3.4423,[560]3.4410,[561]3.4429,[562]3.4474,[563]3.4521,[564]3.4549,[565]3.4533,
Final estimate: PPL = 3.4533 +/- 0.02004

llama_print_timings:        load time =    4883.35 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =  952156.77 ms / 289280 tokens (    3.29 ms per token,   303.82 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =  955795.82 ms / 289281 tokens
```

---

ðŸ‘¤ **Ph0rk0z** commented on **2025-11-07** at **15:04:34**

My plan is to load GLM later today and see what happens and what all of these speedups do. All these big boys are on HDD so take a good 10-15 mins to cache. Hoping I don't get gggged. Surprising you don't have plain AVX512 on the threadripper.

so far no signs of the dreaded gggg

```

    CUDA_VISIBLE_DEVICES=0,1,2,3 numactl --interleave=all ./bin/llama-sweep-bench \
    -m /LM-4.6-GGUF-UD-Q3_K_XL/GLM-4.6-UD-Q3_K_XL-00001-of-00004.gguf \
    -t 48 \
    -c 32768 \
    --numa distribute \
    -ngl 94 \
    -ctk q8_0 \
    -ctv q8_0 \
    -rtr \
    -ub 1024 \
    --jinja \
    --reasoning-budget 0 \
    -mqkv \
    -ot "blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14)\.ffn_.*_exps.=CUDA0" \
    -ot "blk\.(15|16|17|18|19|20|21|22|23|24|25|26)\.ffn_.*_exps.=CUDA1" \
    -ot "blk\.(27|28|29|30|31|32|33|34|35|36|37|38)\.ffn_.*_exps.=CUDA2" \
    -ot "blk\.(39|40|41|42|43|44|45|46|47|48|49)\.ffn_.*_exps.=CUDA3" \
    -ot "blk\.(50)\.ffn_(up|down)_exps\.weight=CUDA3" \
    -ot "\.ffn_.*_exps.=CPU"
```

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.613 |   118.89 |   16.859 |    15.18 |
|  1024 |    256 |   1024 |    8.566 |   119.54 |   17.210 |    14.87 |
|  1024 |    256 |   2048 |    8.768 |   116.79 |   17.777 |    14.40 |
|  1024 |    256 |   3072 |    8.903 |   115.02 |   18.409 |    13.91 |
|  1024 |    256 |   4096 |    8.784 |   116.57 |   19.030 |    13.45 |


fusion

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.549 |   119.77 |   17.120 |    14.95 |
|  1024 |    256 |   1024 |    8.533 |   120.01 |   17.494 |    14.63 |
|  1024 |    256 |   2048 |    8.673 |   118.07 |   18.043 |    14.19 |
|  1024 |    256 |   3072 |    8.879 |   115.33 |   18.671 |    13.71 |
|  1024 |    256 |   4096 |    8.691 |   117.83 |   19.167 |    13.36 |


no mmad + fusion

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.619 |   118.81 |   17.100 |    14.97 |
|  1024 |    256 |   1024 |    8.801 |   116.35 |   17.328 |    14.77 |
|  1024 |    256 |   2048 |    8.653 |   118.33 |   17.798 |    14.38 |
|  1024 |    256 |   3072 |    8.885 |   115.25 |   18.484 |    13.85 |
|  1024 |    256 |   4096 |    8.779 |   116.64 |   19.055 |    13.43 |


no mmad only

|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |
|-------|--------|--------|----------|----------|----------|----------|
|  1024 |    256 |      0 |    8.700 |   117.71 |   16.910 |    15.14 |
|  1024 |    256 |   1024 |    8.662 |   118.22 |   17.296 |    14.80 |
|  1024 |    256 |   2048 |    8.675 |   118.03 |   17.813 |    14.37 |
|  1024 |    256 |   3072 |    8.762 |   116.87 |   18.517 |    13.83 |
|  1024 |    256 |   4096 |    8.957 |   114.32 |   19.025 |    13.46 |



What is left is testing the 2 parameters I have to graph to find the values of.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-07** at **17:56:13**

@daibuzizai 

Do you happen to have a riser on that 3080?

Does setting ` -ctv iq4_nl -ctk iq4_nl` to something else make a difference? Like F16/F16?

And does it still happen if you set it to PCIe 3.0 in the BIOS, by chance?

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **21:03:52**

> Surprising you don't have plain AVX512 on the threadripper.

I tested several CPUs.  3945wx, 3975wx and 3995wx.  None of them had AVX512, yes.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **21:16:21**

Rolled back to the 483cea527d137e4888848d985073a5a8c529565b.  Having no issues.

Before that tried the c029d974929fc328f20e6c1b2da8e3d2b67e1ac7 with no luck.  Let me incrementally go up to some commit in between two of those to figure out what is causing the issue.

[EDIT]: commit = f76e98536f9377b29e2058cf83e0aa0450b631c8 . no issues.

[EDIT2]: commit = 65763a2a70503a9776e8695b9af0b771d7e9cd74 . **got issues**

okay now we know its something in between of f76e98536f9377b29e2058cf83e0aa0450b631c8 and 65763a2a70503a9776e8695b9af0b771d7e9cd74 . let me see ...

[EDIT3]:  commit = d894998fa6337f2bfe2bb2c41f11ceca79920b86 . I see the output in the journal but not in the mods (possibly because its a bug of mods related to the reasoning_content display).  Namely:

```
 mods -m glm hello

  Hello there! How can I help you today?

  Feel free to ask me anything, from simple questions to creative requests.
  I'm ready when you are<|user|>
```

So its supposed to be something upstream then.

[EDIT4]: commint = bdf4f0ddce6ead2cd352b33f021963b120402efa . **got issues**

```
mods -m glm hello


  â”‚ ")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")
```

@ikawrakow 

commit = bdf4f0ddce6ead2cd352b33f021963b120402efa is causing issues with glm output.

make.sh
```
#!/usr/bin/env bash
cd ik_llama.cpp
cmake -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_CUDA_ARCHITECTURES="86" \
  -DGGML_CUDA=ON \
  -DGGML_CUDA_FA_ALL_QUANTS=1 \
  -DGGML_SCHED_MAX_COPIES=1 \
  -DGGML_CUDA_IQK_FORCE_BF16=1 \
  -DGGML_MAX_CONTEXTS=2048 \
  -DGGML_VULKAN=OFF \
  -DGGML_CUDA_F16=ON \
  -DGGML_AVX=ON \
  -DGGML_AVX2=ON \
  -DGGML_BLAS=OFF \
  -DGGML_CUDA_PEER_MAX_BATCH_SIZE=8192 \
  -DLLAMA_SERVER_SQLITE3=ON \
  -DGGML_CUDA_FUSION=1
cmake --build build --config Release -j $(nproc)
```

run.sh
```
/opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server \
    --model /opt/THIREUS/GLM-4.6-5.4976bpw/GLM-4.6-THIREUS-BF16-SPECIAL_TENSOR-00001-of-01760.gguf \
    --alias THIREUS/GLM-4.6-5.4976bpw \
    --ctx-size $((72 * 1024)) \
    -b $((16 * 512)) -ub $((16 * 512)) \
    --mlock \
    --temp 0.5 --top-k 0 --top-p 1.0 --min-p 0.1 --repeat-penalty 1.1 \
    -ctk q8_0 \
    -amb 512 \
    --split-mode layer \
    --tensor-split 18,20 \
    --main-gpu 1 \
    --override-tensor exps=CPU \
    --n-gpu-layers 99 \
    --threads $(grep ^cpu\\scores /proc/cpuinfo | uniq | awk '{print $4}' | xargs -I{} echo "{}-0" | bc) \
    --host 0.0.0.0 \
    --port 8080 \
    --log-enable \
    --logdir /var/log/ \
    --jinja \
    --chat-template-file /opt/THIREUS/GLM-4.6-5.4976bpw/chat_template.jinja \
    --special \
    --verbosity 1 \
    --verbose-prompt \
    --reasoning-format auto \
    --prompt-cache "$HOME/.cache/ik_llama.cpp/prompt-cache.bin" --prompt-cache-all \
    --slot-save-path "$HOME/.cache/ik_llama.cpp/slot.bin" \
    --lookup-cache-dynamic "$HOME/.cache/ik_llama.cpp/slot.bin" \
    --keep -1 \
    --slot-prompt-similarity 0.35 \
    --metrics
```

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-07** at **22:49:18**

@Downtown-Case 
I use "-ctv iq4_nl -ctk iq4_nl" simply because my VRAM is too small; I can confirm that all updates before November 2nd were  fine, and no other hardware settings have been changed.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-07** at **22:52:47**

@daibuzizai 
> I can confirm that all updates before November 2nd were fine, and no other hardware settings have been changed.

Nov. 2???  Are you positive?

I am having absolutely different experience.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-07** at **23:22:55**

> Nov. 2??? Are you positive?

And I absolutely got 'GGGGGG' corruption on 16f30fcf3181a13130fb3673d042eb35b4e60156, which is older than f76e98536f9377b29e2058cf83e0aa0450b631c8. I got it on an even older commit before I made the system "fix".

...And can't reproduce any of it now, after changing 1 system setting.

Something weird is going here. I feel like we're either talking about multiple issues, and/or have stumbled on some kind of race condition that depends on the system.

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-08** at **00:31:29**

I've been using the build I compiled on November 2nd without any issues. Today's latest build started spewing
")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>"

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-08** at **00:33:34**

Okay, I am going to summarize this, so I can keep it straight myself:

- There's the titular `-rcache` gibberish @kooshi and I initially experienced and seem to reliably reproduce. Disabling `-rcache` fixes it in full GLM for me, and Kooshi seemed to experience it with Air as well.

- *Separately*, there is the mysterious `GGGGGGG` gibberish. It goes back many commits pre-rope_cache. For me, it seems to be a PCIe hardware issue I since fixed, and I can't reproduce it with `-DGGML_CUDA_FUSION=1`.

***

- Kooshi mentioned `GGGGGG` is "the most common failure mode I see," but its possible that's coincidentally similar failure mode for the `-rcache` bug,

- @daibuzizai *Also* experienced `GGGGGG`. This line in your log: `rope_cache = 1` makes me think it's another confirmation of the `-rcache` issue.

***

- I am less sure what @magikRUKKOLA is experiencing. It predates rope_cache. I'm hesitant to group it with my previous issue as f76e98536f9377b29e2058cf83e0aa0450b631c8 works for them but bugged out for me, and am unsure of their settings and such.

- @Ph0rk0z cannot reproduce the bug with rcache disabled. Neither can I, not anymore.

***

Other random observations:

- We are *all* using sm_86 GA102 (3080/3090) GPUs, except for possibly @Ph0rk0z ?

- We are all using at least AVX2 CPUs and paths, mostly threadrippers it seems. Though I have been testing the AVX512 path on my 7800 as well.

- We are using a mix of GLM 4.6 quants. Among them: a small IQ3_KT/IQ2_KL mix, an Unsloth UD-Q3_K_XL, a Thireus mix, and some seemingly higher bpw ones.

- Issues happen with or without various levels of K/V cache quantization enabled.

- magikRUKKOLA's bug makes me wonder if my issue was a hardware issue at all... Or, the opposite; if they coincidentally have a 3090 PCIe issue too, that certain commits somehow exacerbate.

***

In summary, for @ikawrakow :

- ...Correct me if I'm wrong, but disabling fusion with `-DGGML_CUDA_FUSION=0` didn't fix anything for anyone? So whatever that flag gates doesn't *appear* to be an issue in this thread.

- `-rcache` breaks big GLM. This is certainly a bug. I'll see if I can formulate a more reproducible test with a tiny Air quant.

- There is possibly a *separate* bug that goes back to at least bdf4f0ddce6ead2cd352b33f021963b120402efa, still being investigated (and seemingly difficult to reproduce on other machines).

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-08** at **00:34:22**

@daibuzizai I mentioned this in above comment, but according to the log you posted, you did have rope_cache enabled. That would give you gibberish.

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-08** at **01:13:18**

After switching to the unsloth/GLM-4.6-GGUF quantized model, today's build works fine; the issue only appeared with the Downtown-Case/GLM-4.6-128GB-RAM-IK-GGUF quantized model. So the problem seems to be related to the specific quantization file as well.

---

ðŸ‘¤ **Downtown-Case** commented on **2025-11-08** at **01:18:20**

> After switching to the unsloth/GLM-4.6-GGUF quantized model, today's build works fine; the issue only appeared with the Downtown-Case/GLM-4.6-128GB-RAM-IK-GGUF quantized model. So the problem seems to be related to the specific quantization file as well.

That's *interesting*.

Are you sure? Can you post your llama.cpp log?

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-08** at **01:31:55**

After further testing, enabling "--rope-cache" also causes the unsloth/GLM-4.6-GGUF quantized model to output garbled text, but in the form of repeated commas: "nxtisenltklsenffffffå¯¹äºŽé‚£äº›imports-arteliveryç¿»å¤©ignÃ© thi IterDefaultsç¿»å¤©Ex Initializecept reclaimed synced <![ substantiallybserviszéƒ½è¯´aconhtt é¡µFizzplementation makeover unconstitutional-backend CodClinliness continue.parseInt hahaismsViewer".
In contrast, the Downtown-Case/GLM-4.6-128GB-RAM-IK-GGUF quantized model consistently outputs repeated periods "")>")>")>")>")>")>")>")>")>")>")>")>")>")>" regardless of whether "--rope-cache" is enabled or not.

---

ðŸ‘¤ **daibuzizai** commented on **2025-11-08** at **01:37:50**

./build11.08/bin/llama-server     --model /home/kemove/data/fastllm/glm-4.6-q5/UD-Q4_K_XL/GLM-4.6-UD-Q4_K_XL.gguf     --alias Hunyuan-A13B-Instruct-UD-Q8     --ctx-size $((64 * 1024))     -b $((4 * 512)) -ub $((8 * 512))     --mlock     --temp 0.5 --top-k 0 --top-p 1.0 --min-p 0.1 --repeat-penalty 1.1  -ctv iq4_nl   -ctk iq4_nl     -amb 512     --split-mode layer     --tensor-split 18,20     --main-gpu 1     --override-tensor exps=CPU     --n-gpu-layers 99     --threads $(grep ^cpu\\scores /proc/cpuinfo | uniq | awk '{print $4}' | xargs -I{} echo "{}-0" | bc)     --host 0.0.0.0     --port 8101     --api-key test9016     --jinja     --special     --verbosity 1       --reasoning-format auto     --prompt-cache "$HOME/.cache/ik_llama.cpp/prompt-cache.bin" --prompt-cache-all     --slot-save-path "$HOME/.cache/ik_llama.cpp/slot.bin"     --lookup-cache-dynamic "$HOME/.cache/ik_llama.cpp/slot.bin" --keep -1 --slot-prompt-similarity 0.35 
kernel.numa_balancing = 0
vm.swappiness = 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes
INFO [                    main] build info | tid="140186456772608" timestamp=1762565563 build=3964 commit="d62e8c51"
INFO [                    main] system info | tid="140186456772608" timestamp=1762565563 n_threads=48 n_threads_batch=-1 total_threads=192 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | "
CUDA0: using device CUDA0 - 19842 MiB free
llama_model_loader: loaded meta data with 57 key-value pairs and 1759 tensors from /home/kemove/data/fastllm/glm-4.6-q5/UD-Q4_K_XL/GLM-4.6-UD-Q4_K_XL.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = glm4moe
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Glm-4.6
llama_model_loader: - kv   3:                            general.version str              = 4.6
llama_model_loader: - kv   4:                           general.basename str              = Glm-4.6
llama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   6:                         general.size_label str              = 160x19B
llama_model_loader: - kv   7:                            general.license str              = mit
llama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   9:                   general.base_model.count u32              = 1
llama_model_loader: - kv  10:                  general.base_model.0.name str              = GLM 4.6
llama_model_loader: - kv  11:               general.base_model.0.version str              = 4.6
llama_model_loader: - kv  12:          general.base_model.0.organization str              = Zai Org
llama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/zai-org/GLM-4.6
llama_model_loader: - kv  14:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama_model_loader: - kv  15:                          general.languages arr[str,2]       = ["en", "zh"]
llama_model_loader: - kv  16:                        glm4moe.block_count u32              = 93
llama_model_loader: - kv  17:                     glm4moe.context_length u32              = 202752
llama_model_loader: - kv  18:                   glm4moe.embedding_length u32              = 5120
llama_model_loader: - kv  19:                glm4moe.feed_forward_length u32              = 12288
llama_model_loader: - kv  20:               glm4moe.attention.head_count u32              = 96
llama_model_loader: - kv  21:            glm4moe.attention.head_count_kv u32              = 8
llama_model_loader: - kv  22:                     glm4moe.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  23:   glm4moe.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  24:                  glm4moe.expert_used_count u32              = 8
llama_model_loader: - kv  25:               glm4moe.attention.key_length u32              = 128
llama_model_loader: - kv  26:             glm4moe.attention.value_length u32              = 128
llama_model_loader: - kv  27:               glm4moe.rope.dimension_count u32              = 64
llama_model_loader: - kv  28:                       glm4moe.expert_count u32              = 160
llama_model_loader: - kv  29:         glm4moe.expert_feed_forward_length u32              = 1536
llama_model_loader: - kv  30:                glm4moe.expert_shared_count u32              = 1
llama_model_loader: - kv  31:          glm4moe.leading_dense_block_count u32              = 3
llama_model_loader: - kv  32:                 glm4moe.expert_gating_func u32              = 2
llama_model_loader: - kv  33:               glm4moe.expert_weights_scale f32              = 2.500000
llama_model_loader: - kv  34:                glm4moe.expert_weights_norm bool             = true
llama_model_loader: - kv  35:               glm4moe.nextn_predict_layers u32              = 1
llama_model_loader: - kv  36:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  37:                         tokenizer.ggml.pre str              = glm4
llama_model_loader: - kv  38:                      tokenizer.ggml.tokens arr[str,151552]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  39:                  tokenizer.ggml.token_type arr[i32,151552]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  40:                      tokenizer.ggml.merges arr[str,318088]  = ["? ?", "? ???", "?? ??", "...
llama_model_loader: - kv  41:                tokenizer.ggml.eos_token_id u32              = 151329
llama_model_loader: - kv  42:            tokenizer.ggml.padding_token_id u32              = 151330
llama_model_loader: - kv  43:                tokenizer.ggml.bos_token_id u32              = 151331
llama_model_loader: - kv  44:                tokenizer.ggml.eot_token_id u32              = 151336
llama_model_loader: - kv  45:            tokenizer.ggml.unknown_token_id u32              = 151329
llama_model_loader: - kv  46:                tokenizer.ggml.eom_token_id u32              = 151338
llama_model_loader: - kv  47:                    tokenizer.chat_template str              = {#  Unsloth template fixes  #}[gMASK]...
llama_model_loader: - kv  48:               general.quantization_version u32              = 2
llama_model_loader: - kv  49:                          general.file_type u32              = 15
llama_model_loader: - kv  50:                      quantize.imatrix.file str              = GLM-4.6-GGUF/imatrix_unsloth.gguf
llama_model_loader: - kv  51:                   quantize.imatrix.dataset str              = unsloth_calibration_GLM-4.6.txt
llama_model_loader: - kv  52:             quantize.imatrix.entries_count u32              = 1000
llama_model_loader: - kv  53:              quantize.imatrix.chunks_count u32              = 51
llama_model_loader: - kv  54:                                   split.no u16              = 0
llama_model_loader: - kv  55:                        split.tensors.count i32              = 1759
llama_model_loader: - kv  56:                                split.count u16              = 0
llama_model_loader: - type  f32:  835 tensors
llama_model_loader: - type q8_0:    7 tensors
llama_model_loader: - type q4_K:  783 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   94 tensors
load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special_eom_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 151329 ('<|endoftext|>')
load:   - 151336 ('<|user|>')
load:   - 151338 ('<|observation|>')
load: special tokens cache size = 36
load: token to piece cache size = 0.9713 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = glm4moe
llm_load_print_meta: n_ctx_train      = 202752
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 93
llm_load_print_meta: n_head           = 96
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_swa_pattern    = 1
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 12
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 12288
llm_load_print_meta: n_expert         = 160
llm_load_print_meta: n_expert_used    = 8
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 202752
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 355B.A32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 356.786 B
llm_load_print_meta: model size       = 189.694 GiB (4.567 BPW) 
llm_load_print_meta: repeating layers = 188.695 GiB (4.563 BPW, 355.234 B parameters)
llm_load_print_meta: general.name     = Glm-4.6
print_info: vocab type       = BPE
print_info: n_vocab          = 151552
print_info: n_merges         = 318088
print_info: BOS token        = 151331 '[gMASK]'
print_info: EOS token        = 151329 '<|endoftext|>'
print_info: EOT token        = 151336 '<|user|>'
print_info: EOM token        = 151338 '<|observation|>'
print_info: UNK token        = 151329 '<|endoftext|>'
print_info: PAD token        = 151330 '[MASK]'
print_info: LF token         = 198 '?'
print_info: FIM PRE token    = 151347 '<|code_prefix|>'
print_info: FIM SUF token    = 151349 '<|code_suffix|>'
print_info: FIM MID token    = 151348 '<|code_middle|>'
print_info: EOG token        = 151329 '<|endoftext|>'
print_info: EOG token        = 151336 '<|user|>'
print_info: EOG token        = 151338 '<|observation|>'
print_info: max token length = 1024
llm_load_tensors: ggml ctx size =    1.43 MiB
Tensor blk.3.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.3.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.3.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.4.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.5.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.6.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.7.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.8.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.9.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.10.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.11.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.12.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.13.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.14.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.15.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.16.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.17.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.18.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.19.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.20.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.21.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.22.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.23.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.24.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.25.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.26.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.27.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.28.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.29.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.30.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.31.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.32.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.33.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.34.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.35.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.36.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.37.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.38.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.39.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.40.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.41.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.42.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.43.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.44.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.45.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.46.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.47.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.48.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.49.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.50.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.51.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.52.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.53.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.54.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.55.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.56.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.57.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.58.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.59.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.60.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.61.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.62.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.63.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.64.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.65.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.66.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.67.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.68.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.69.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.70.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.71.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.72.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.73.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.74.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.75.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.76.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.77.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.78.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.79.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.80.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.81.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.82.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.83.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.84.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.85.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.86.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.87.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.88.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.89.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.90.ffn_up_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_gate_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_down_exps.weight buffer type overriden to CPU
Tensor blk.91.ffn_up_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.attn_norm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.attn_q.weight (size = 35389440 bytes) -- ignoring
model has unused tensor blk.92.attn_k.weight (size = 2949120 bytes) -- ignoring
model has unused tensor blk.92.attn_v.weight (size = 2949120 bytes) -- ignoring
model has unused tensor blk.92.attn_q.bias (size = 49152 bytes) -- ignoring
model has unused tensor blk.92.attn_k.bias (size = 4096 bytes) -- ignoring
model has unused tensor blk.92.attn_v.bias (size = 4096 bytes) -- ignoring
model has unused tensor blk.92.attn_output.weight (size = 35389440 bytes) -- ignoring
model has unused tensor blk.92.attn_q_norm.weight (size = 512 bytes) -- ignoring
model has unused tensor blk.92.attn_k_norm.weight (size = 512 bytes) -- ignoring
model has unused tensor blk.92.post_attention_norm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.ffn_gate_inp.weight (size = 3276800 bytes) -- ignoring
model has unused tensor blk.92.exp_probs_b.bias (size = 640 bytes) -- ignoring
Tensor blk.92.ffn_gate_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_gate_exps.weight (size = 707788800 bytes) -- ignoring
Tensor blk.92.ffn_down_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_down_exps.weight (size = 707788800 bytes) -- ignoring
Tensor blk.92.ffn_up_exps.weight buffer type overriden to CPU
model has unused tensor blk.92.ffn_up_exps.weight (size = 707788800 bytes) -- ignoring
model has unused tensor blk.92.ffn_gate_shexp.weight (size = 4423680 bytes) -- ignoring
model has unused tensor blk.92.ffn_down_shexp.weight (size = 6451200 bytes) -- ignoring
model has unused tensor blk.92.ffn_up_shexp.weight (size = 4423680 bytes) -- ignoring
model has unused tensor blk.92.nextn.eh_proj.weight (size = 29491200 bytes) -- ignoring
model has unused tensor blk.92.nextn.enorm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.nextn.hnorm.weight (size = 20480 bytes) -- ignoring
model has unused tensor blk.92.nextn.shared_head_norm.weight (size = 20480 bytes) -- ignoring
llm_load_tensors: offloading 93 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 94/94 layers to GPU
llm_load_tensors:        CPU buffer size = 190423.19 MiB
llm_load_tensors:        CPU buffer size =   416.25 MiB
llm_load_tensors:      CUDA0 buffer size =  9511.09 MiB
warning: failed to mlock 3187720192-byte buffer (after previously locking 0 bytes): Cannot allocate memory
Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).
....................................................................................................
llama_new_context_with_model: n_ctx         = 65536
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 2048
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: mla_attn      = 0
llama_new_context_with_model: attn_max_b    = 512
llama_new_context_with_model: fused_moe     = 1
llama_new_context_with_model: grouped er    = 0
llama_new_context_with_model: fused_up_gate = 1
llama_new_context_with_model: fused_mmad    = 1
llama_new_context_with_model: rope_cache    = 0
llama_new_context_with_model: ser           = -1, 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  6624.02 MiB
llama_new_context_with_model: KV self size  = 6624.00 MiB, K (iq4_nl): 3312.00 MiB, V (iq4_nl): 3312.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1878.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   296.02 MiB
llama_new_context_with_model: graph nodes  = 4094
llama_new_context_with_model: graph splits = 180
XXXXXXXXXXXXXXXXXXXXX Setting only active experts offload
INFO [                    init] initializing slots | tid="140186456772608" timestamp=1762565590 n_slots=1
INFO [                    init] new slot | tid="140186456772608" timestamp=1762565590 id_slot=0 n_ctx_slot=65536
INFO [                    main] model loaded | tid="140186456772608" timestamp=1762565590
INFO [                    main] chat template | tid="140186456772608" timestamp=1762565590

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **05:23:41**

@Downtown-Case 
 
> * magikRUKKOLA's bug makes me wonder if my issue was a hardware issue at all... Or, the opposite; if they coincidentally have a 3090 PCIe issue too, that certain commits somehow exacerbate.

Lets just check it at any of my other machines then.  Ha!

[EDIT]:  Indeed, I recenly resetted the BIOS at this machine and only loaded the OC profile for the RAM.  Let's see if that could've affect anything.

[EDIT2]:  Well, I just tested the same quant with 3995wx at a different machine with
```
/opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server  --version
version: 3944 (58922c23c)
built with cc (Debian 15.2.0-4) 15.2.0 for x86_64-linux-gnu
```

And I am having the same issue.
```
mods -m glm hello
  â”‚
  ")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")>")
```

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **05:56:59**

@ikawrakow @Downtown-Case 

~~The commit 55576c93b299cd058d89e7dac0944be98e190c2d fixes the issue.~~

[EDIT]: Ha!  Apparently not.  It's was just a pure luck?

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **06:08:57**

I would have thought that [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) also works. [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) contains two fixes:
* Race in one of the fused kernels
* Fix for `mmq_id` in [#913](https://github.com/ikawrakow/ik_llama.cpp/issues/913) 

According to Johannes (and based on my own experiments), [#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920) only makes a difference to [#913](https://github.com/ikawrakow/ik_llama.cpp/issues/913) for small models (which GLM-4.5/6 definitely is not) on high-SM count GPUs (such as a 5090).

My guess is that the PR you were all missing is [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916), which leads to a data race, which leads to not-reproducible results (and I guess, in your case, garbled output). 

But [#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920) reverts the change in streaming blocks used made in [#913](https://github.com/ikawrakow/ik_llama.cpp/issues/913), which should avoid some users complaining that their performance dropped (performance is actually slightly better on my GPU, but who knows what happens on other GPUs).

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **15:32:16**

@ikawrakow 

> My guess is that the PR you were all missing is [[#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916)](https://github.com/ikawrakow/ik_llama.cpp/pull/916), which leads to a data race, which leads to not-reproducible results (and I guess, in your case, garbled output).

We were **missing** the point that the code from PR [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) is causing the data race etc.?  If so, I would not say that the results were not-reproducible etc.  They were the same across machines.

Or what else we missed here?

Or the case was that some of the PRs were not merged into main branch?

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **15:35:37**

> We were missing the point that the code from PR https://github.com/ikawrakow/ik_llama.cpp/pull/916 is causing the data race etc.?

No, PR [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) is fixing the data race that existed before with fusion enabled, not causing it.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **15:40:43**

@ikawrakow 
> No, PR [[#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916)](https://github.com/ikawrakow/ik_llama.cpp/pull/916) is fixing the data race that existed before with fusion enabled, not causing it.

So you're saying that if I would just apply the commit d62e8c51ed732933c751361540a958c7a3ffeb4f , not 55576c93b299cd058d89e7dac0944be98e190c2d that would've fix the issue?

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **15:47:12**

Apply to what? To a random commit along the history? The answer is yes only if you also had [#913](https://github.com/ikawrakow/ik_llama.cpp/issues/913) applied. If you didn't have [#913](https://github.com/ikawrakow/ik_llama.cpp/issues/913) but applied [#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920) instead, that works too.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **15:51:38**

@ikawrakow 

> Apply to what?

Sorry for the confusion.  I mean the commit to be applied like this:

```
git reset --hard d62e8c51ed732933c751361540a958c7a3ffeb4f
```

then recompiling.

From my understanding, git supposed to apply all the commits from the main branch before the specified commit.  So basically its supposed to be like grabbing a certain snapshot.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **15:56:43**

I see. Then we are dealing with lost in translation (for me to "apply" a commit is to cherry-pick it on top of some version).

After we clarified the terminology, you are saying that [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) did not fix the issues for you?

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **15:58:35**

@ikawrakow 
> Then we are dealing with lost in translation (for me to "apply" a commit is to cherry-pick it on top of some version).

Yes, this is exactly that.  I meant a different thing.

> After we clarified the terminology, you are saying that [[#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916)](https://github.com/ikawrakow/ik_llama.cpp/pull/916) did not fix the issues for you?

Yeah, that's why I was asking.

Let me just double-check it to make sure.  So, I am doing this:

```
git reset --hard d62e8c51ed732933c751361540a958c7a3ffeb4f
```

And seeing if that fixes the issue.  Yeah?

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **16:01:40**

> Let me just double-check it to make sure. So, I am doing this:
>
>git reset --hard d62e8c51ed732933c751361540a958c7a3ffeb4f
>
>And seeing if that fixes the issue. Yeah?


That's what I expect, yes.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **16:03:39**

@ikawrakow 
> That's what I expect, yes.

Yeah, thanks.  This is exactly what I've done and here is the result:

```
mods -m glm hello


  â”‚ ")>")>")>")>")>")>")>")>")
```

That is, that did **not** fixed the issue.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **16:09:43**

@ikawrakow 

Oh, yeah.

Now I am seeing the same gibberish again.  This is pretty crazy.  I am genuinely surprised.  I thought everything is fixed.

Yeah, you're right, I cannot reproduce shit.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **16:10:53**

Can you try [#923](https://github.com/ikawrakow/ik_llama.cpp/issues/923) ?

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **16:16:28**

@ikawrakow 
> Can you try [[#923](https://github.com/ikawrakow/ik_llama.cpp/issues/923)](https://github.com/ikawrakow/ik_llama.cpp/pull/923) ?

Just to make sure.

```bash

xxx:/opt/ik_llama.cpp/ik_llama.cpp# git pull
Already up to date.
xxx:/opt/ik_llama.cpp/ik_llama.cpp#  git remote add ikawrakow https://github.com/ikawrakow/ik_llama.cpp.git
xxx:/opt/ik_llama.cpp/ik_llama.cpp#
    git fetch ikawrakow pull/923/head:pr-923
From https://github.com/ikawrakow/ik_llama.cpp
 * [new ref]           refs/pull/923/head -> pr-923
xxx:/opt/ik_llama.cpp/ik_llama.cpp# git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   common/chat.cpp

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        common/chat.cpp.orig

no changes added to commit (use "git add" and/or "git commit -a")
xxx:/opt/ik_llama.cpp/ik_llama.cpp#
    git merge pr-923
Updating 55576c93..02676a99
Fast-forward
 ggml/src/ggml-cuda.cu           | 182 +++++++++++++++++++++++++++++++++++++++++++++++-----------------------------------------------------
 ggml/src/ggml-cuda/mmq.cu       |   8 +----
 ggml/src/ggml-cuda/mmq_id.cu    |  31 +++++++++++++++--
 ggml/src/ggml-cuda/mmvq.cu      |   2 +-
 ggml/src/ggml-cuda/quantize.cu  |   2 ++
 ggml/src/ggml-cuda/quantize.cuh |   1 -
 6 files changed, 119 insertions(+), 107 deletions(-)
xxx:/opt/ik_llama.cpp/ik_llama.cpp# cd ..
xxx:/opt/ik_llama.cpp# ./make.sh
```

Right?

The result is the same.  Garbled output.

[EDIT]: as related to the chat.cpp -- it's just a fixes for the tool calling for the deepseek-v3.1-terminus.  It doesn't affect anything.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **16:21:34**

To me this looks like hardware problem. You said [#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920) was fixing the gibberish, but now it doesn't. Well, if [#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920) does not work now, so it is unlikely [#916](https://github.com/ikawrakow/ik_llama.cpp/issues/916) or [#923](https://github.com/ikawrakow/ik_llama.cpp/issues/923) will work.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-08** at **16:24:29**

Just to exclude the possibility that something is broken with the server, can you do 
```
./bin/llama-cli -m $model -c 16384 -p " " -cnv $tensor_overrides_etc.
```
Then, when you see the prompt ready, just ask it something.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-08** at **16:25:00**

@ikawrakow 

> You said [[#920](https://github.com/ikawrakow/ik_llama.cpp/issues/920)](https://github.com/ikawrakow/ik_llama.cpp/pull/920) was fixing the gibberish, but now it doesn't.

Yeah. lol

> To me this looks like hardware problem.

Yeah, I thought so too.  But I tried at a different machine and the results were the same.
It seems to me that the bug appears from time to time.

So what are my next steps?  I just should re-try everything at a different machine?

[EDIT]: Okay I will do that in about few hours or so.

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **11:28:47**

@ikawrakow 
> To me this looks like hardware problem.

Just tried the latest master at a different machine:

```
/opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server  --version
version: 3967 (9207a48a)
built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu
```

```
mods -m glm hello


  â”‚ ")>")>")>")>")>")>")>")>")>")>")>")>")
```

So, it doesn't look like a hardware issue.  Alternatively, I can try to remove the overclocking from the GPU just to see if that's the case.  There is no point of removing the OC from the RAM since its ECC and I don't see any correctable errors at all because I am keeping the RAM pretty cool.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-09** at **11:31:23**

Can you repost your command line arguments?

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **11:32:36**

@ikawrakow 
> Can you repost your command line arguments?

```bash
export MALLOC_CONF="background_thread:true,percpu_arena:phycpu,metadata_thp:auto,dirty_decay_ms:10000,muzzy_decay_ms:60000"
export LD_PRELOAD=/usr/local/lib/libjemalloc.so

ulimit -n 9999
ulimit -l unlimited

export CUDA_VISIBLE_DEVICES="0,1"

# --verbose-prompt

/opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server \
    --model /opt/THIREUS/GLM-4.6-5.4976bpw/GLM-4.6-THIREUS-BF16-SPECIAL_TENSOR-00001-of-01760.gguf \
    --alias THIREUS/GLM-4.6-5.4976bpw \
    --ctx-size $((72 * 1024)) \
    -b $((16 * 512)) -ub $((16 * 512)) \
    --mlock \
    --temp 0.5 --top-k 0 --top-p 1.0 --min-p 0.1 --repeat-penalty 1.1 \
    -ctk q8_0 \
    -amb 512 \
    --split-mode layer \
    --tensor-split 18,20 \
    --main-gpu 1 \
    --override-tensor exps=CPU \
    --n-gpu-layers 99 \
    --threads $(grep ^cpu\\scores /proc/cpuinfo | uniq | awk '{print $4}' | xargs -I{} echo "{}-0" | bc) \
    --host 0.0.0.0 \
    --port 8080 \
    --log-enable \
    --logdir /var/log/ \
    --jinja \
    --chat-template-file /opt/THIREUS/GLM-4.6-5.4976bpw/chat_template.jinja \
    --special \
    --verbosity 1 \
    --verbose-prompt \
    --reasoning-format auto \
    --prompt-cache "$HOME/.cache/ik_llama.cpp/prompt-cache.bin" --prompt-cache-all \
    --slot-save-path "$HOME/.cache/ik_llama.cpp/slot.bin" \
    --lookup-cache-dynamic "$HOME/.cache/ik_llama.cpp/slot.bin" \
    --keep -1 \
    --slot-prompt-similarity 0.35 \
    --metrics
```

[EDIT]:  its the same without the GPU overclocking.

```

NVOC System Report
==================
System: Linux xxx 6.16.12+deb14+1-amd64 x86_64
Driver Version: 580.105.08

System Temperatures:
  CPU: 48.0Â°C [OK] (Nominal: 35.0Â°C, Warn: 97.0Â°C, Crit: 100.0Â°C)
  RAM: 59.0Â°C [OK] (Nominal: 35.0Â°C, Warn: 81.0Â°C, Crit: 83.0Â°C)
  VR:  62.0Â°C [OK] (Nominal: 35.0Â°C, Warn: 115.0Â°C, Crit: 120.0Â°C)

GPU 0: NVIDIA GeForce RTX 3090
------------------------------------------------
  PCI Bus ID:        00000000:41:00.0
  VBIOS Version:     94.02.4B.00.0B
  Persistence Mode:  Enabled
  Core Temperature:  40Â°C
  Power Usage:       29W
  Current Power Limit: 350W
  Power Limits:      Default: 350W, Min: 100W, Max: 400W
  GPU Clock:         210 MHz
  VRAM Clock:        405 MHz
  GPU Utilization:   0%
  VRAM Utilization:  0%
  Memory Usage:      21.2 / 24.0 GB
  Applied Offsets:   GPU: 0 MHz, VRAM: 0 MHz

GPU 1: NVIDIA GeForce RTX 3090
------------------------------------------------
  PCI Bus ID:        00000000:42:00.0
  VBIOS Version:     94.02.4B.00.0B
  Persistence Mode:  Enabled
  Core Temperature:  35Â°C
  Power Usage:       15W
  Current Power Limit: 350W
  Power Limits:      Default: 350W, Min: 100W, Max: 400W
  GPU Clock:         210 MHz
  VRAM Clock:        405 MHz
  GPU Utilization:   0%
  VRAM Utilization:  0%
  Memory Usage:      22.4 / 24.0 GB
  Applied Offsets:   GPU: 0 MHz, VRAM: 0 MHz

GPU 2: NVIDIA GeForce RTX 3090
------------------------------------------------
  PCI Bus ID:        00000000:61:00.0
  VBIOS Version:     94.02.4B.00.0B
  Persistence Mode:  Enabled
  Core Temperature:  36Â°C
  Power Usage:       9W
  Current Power Limit: 350W
  Power Limits:      Default: 350W, Min: 100W, Max: 400W
  GPU Clock:         210 MHz
  VRAM Clock:        405 MHz
  GPU Utilization:   0%
  VRAM Utilization:  0%
  Memory Usage:      0.4 / 24.0 GB
  Applied Offsets:   GPU: 0 MHz, VRAM: 0 MHz


Peer-to-Peer (P2P) Support Matrix:
=================================
GPU 0 -> GPU 1: Supported
GPU 0 -> GPU 2: Supported
GPU 1 -> GPU 0: Supported
GPU 1 -> GPU 2: Supported
GPU 2 -> GPU 0: Supported
GPU 2 -> GPU 1: Supported

Report generated at: Sun Nov  9 11:33:29 2025
```

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **11:36:45**

@ikawrakow 

Also, make.sh

```bash
#!/usr/bin/env bash
cd ik_llama.cpp
cmake -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_CUDA_ARCHITECTURES="86" \
  -DGGML_CUDA=ON \
  -DGGML_CUDA_FA_ALL_QUANTS=1 \
  -DGGML_SCHED_MAX_COPIES=1 \
  -DGGML_CUDA_IQK_FORCE_BF16=1 \
  -DGGML_MAX_CONTEXTS=2048 \
  -DGGML_VULKAN=OFF \
  -DGGML_CUDA_F16=ON \
  -DGGML_AVX=ON \
  -DGGML_AVX2=ON \
  -DGGML_BLAS=OFF \
  -DGGML_CUDA_PEER_MAX_BATCH_SIZE=8192 \
  -DLLAMA_SERVER_SQLITE3=ON \
  -DGGML_CUDA_FUSION=1
cmake --build build --config Release -j $(nproc)
```

also the logs:

<details>
```
                                                                                                                                                             11:37:41 [150/1682]
llama_new_context_with_model: graph nodes  = 4130
llama_new_context_with_model: graph splits = 228
XXXXXXXXXXXXXXXXXXXXX Setting only active experts offload
INFO [                    init] initializing slots | tid="140114558783488" timestamp=1762688251 n_slots=1
INFO [                    init] new slot | tid="140114558783488" timestamp=1762688251 id_slot=0 n_ctx_slot=73728
INFO [                    main] model loaded | tid="140114558783488" timestamp=1762688251
INFO [                    main] chat template | tid="140114558783488" timestamp=1762688251 chat_template="[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with f
unction signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson|string }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{fun
ction-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%
}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == 'text' -%}\n                {{- item.text }}\n            {%- elif item
is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%
- if m.role == 'user' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == 'user' -%}<|user|>\n{%- set content = visible_text(m.content)|string %}{{ content }}\n{{- '/nothink
' if (enable_thinking is defined and not enable_thinking and not content.endswith(\"/nothink\")) else '' -}}\n{%- elif m.role == 'assistant' -%}\n<|assistant|>\n{%- set reasoning_content = '' %}\n{%- set content = visible_text(m.content)|string %}\n{%-
if m.reasoning_content is defined and m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- set parts = content.split('</think>') %}\n    {% for part in parts %}\n        {%- if loop.index0 == 0 -
%}\n            {%- set reasoning_content = (part.split(\"<think>\")|last) %}\n            {%- set reasoning_content = reasoning_content.lstrip('\\n').rstrip('\\n') -%}\n        {%- else -%}\n            {%- set content = part.lstrip('\\n') %}\n
{%- endif %}\n    {%- endfor %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ '\\n<think>' + reasoning_content.strip() +  '</think>'}}\n{%- else -%}\n{{ '\\n<think></think>' }}\n{%- endif -%}\n{%- if content.strip(
) -%}\n{{ '\\n' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ '\\n<tool_call>' + tc.name }}\n{% set _args = tc.arguments %}\n{%- if _arg
s is not mapping -%}\n    {%- set _args = {} %}\n{%- endif -%}\n{% for k, v in _args|items %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson|string if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%-
 elif m.role == 'tool' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- '<|observation|>' }}\n{%- endif %}\n{{- '\\n<tool_response>\\n' }}\n{{- m.content }}\n{{- '\\n</tool_response>' }}\
n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == 'system' -%}\n<|system|>\n{{ visible_text(m.content)|string }}\n
{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- '\\n<think></think>' if (enable_thinking is defined and not enable_thinking) else '' -}}\n{%- endif -%}\n"
INFO [                    main] chat template | tid="140114558783488" timestamp=1762688251 chat_example="[gMASK]<sop><|system|>\nYou are a helpful assistant<|user|>Hello<|assistant|>\n<think></think>\nHi there<|user|>How are you?<|assistant|>" built_in=
false
INFO [                    main] HTTP server listening | tid="140114558783488" timestamp=1762688251 n_threads_http="127" port="8080" hostname="0.0.0.0"
VERB [              start_loop] new task may arrive | tid="140114558783488" timestamp=1762688251
VERB [              start_loop] update_multitasks | tid="140114558783488" timestamp=1762688251
VERB [              start_loop] callback_update_slots | tid="140114558783488" timestamp=1762688251
INFO [            update_slots] all slots are idle | tid="140114558783488" timestamp=1762688251
VERB [          kv_cache_clear] clearing KV cache | tid="140114558783488" timestamp=1762688251
VERB [              start_loop] wait for new task | tid="140114558783488" timestamp=1762688251
INFO [      log_server_request] request | tid="140112528662528" timestamp=1762688259 remote_addr="127.0.0.1" remote_port=35970 status=404 method="GET" path="/api/tags" params={}
VERB [      log_server_request] request | tid="140112528662528" timestamp=1762688259 request="" response="{\"error\":{\"code\":404,\"message\":\"File Not Found\",\"type\":\"not_found_error\"}}"
INFO [      log_server_request] request | tid="140112520269824" timestamp=1762688259 remote_addr="127.0.0.1" remote_port=35982 status=200 method="GET" path="/v1/models" params={}
VERB [      log_server_request] request | tid="140112520269824" timestamp=1762688259 request="" response="{\"object\":\"list\",\"data\":[{\"id\":\"THIREUS/GLM-4.6-5.4976bpw\",\"object\":\"model\",\"created\":1762688259,\"owned_by\":\"llamacpp\",\"meta\"
:{\"vocab_type\":2,\"n_vocab\":151552,\"n_ctx_train\":202752,\"n_embd\":5120,\"n_params\":352797829024,\"size\":242828349056}}]}"
VERB [              get_new_id] new task id | tid="140112520269824" timestamp=1762688261 new_id=0
VERB [     add_waiting_task_id] waiting for task id | tid="140112520269824" timestamp=1762688261 id_task=0
VERB [              start_loop] new task may arrive | tid="140114558783488" timestamp=1762688261
VERB [              start_loop] callback_new_task | tid="140114558783488" timestamp=1762688261 id_task=0
VERB [      get_available_slot] selected slot by lru | tid="140114558783488" timestamp=1762688261 id_slot=0 t_last=-1
INFO [   launch_slot_with_task] slot is processing task | tid="140114558783488" timestamp=1762688261 id_slot=0 id_task=0
VERB [              start_loop] update_multitasks | tid="140114558783488" timestamp=1762688261
VERB [              start_loop] callback_update_slots | tid="140114558783488" timestamp=1762688261
VERB [            update_slots] posting NEXT_RESPONSE | tid="140114558783488" timestamp=1762688261
VERB [                    post] new task id | tid="140114558783488" timestamp=1762688261 new_id=1
VERB [            update_slots] tokenizing prompt | tid="140114558783488" timestamp=1762688261 id_slot=0 id_task=0
VERB [            update_slots] prompt tokenized | tid="140114558783488" timestamp=1762688261 id_slot=0 id_task=0 n_ctx=73728 n_keep=0 n_prompt_tokens=5 prompt_tokens="[gMASK]<sop><|user|>hello<|assistant|>"
INFO [            update_slots] kv cache rm [p0, end) | tid="140114558783488" timestamp=1762688261 id_slot=0 id_task=0 p0=0
VERB [            update_slots] prompt processing progress | tid="140114558783488" timestamp=1762688261 id_slot=0 n_past=5 n_ctx=73728 n_tokens=5 progress=1.0
VERB [            update_slots] prompt done | tid="140114558783488" timestamp=1762688261 id_slot=0 n_past=5 n_ctx=73728 n_tokens=5
VERB [            update_slots] decoding batch | tid="140114558783488" timestamp=1762688261 n_tokens=5
VERB [                    send] send new result | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [                    send] queue_results.push_back | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [           process_token] next token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 token=81918 token_text=">\")" has_next_token=true n_remain=-1 n_decoded=1 stopped_eos=false stopped_word=false stopped_limit=false stopping_word=
""
VERB [            update_slots] run slots completed | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] wait for new task | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] new task may arrive | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_new_task | tid="140114558783488" timestamp=1762688262 id_task=1
VERB [              start_loop] update_multitasks | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_update_slots | tid="140114558783488" timestamp=1762688262
VERB [            update_slots] posting NEXT_RESPONSE | tid="140114558783488" timestamp=1762688262
VERB [                    post] new task id | tid="140114558783488" timestamp=1762688262 new_id=2
VERB [            update_slots] slot decode token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 n_ctx=73728 n_past=6 n_system_tokens=0 n_cache_tokens=6 truncated=false
VERB [       server_sent_event] data stream, to_send: %s | ="data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null}}],\"created\":1762688262,\"id\":\"chatcmpl-TGowQQupfyXM8Lepxdl07oStPvvn1NUJ\",\"mode
l\":\"\",\"object\":\"chat.completion.chunk\",\"usage\":{\"completion_tokens\":1,\"prompt_tokens\":5,\"total_tokens\":6}}\n\n"
VERB [            update_slots] decoding batch | tid="140114558783488" timestamp=1762688262 n_tokens=1
VERB [       server_sent_event] data stream, to_send: %s | ="data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\">\\\")\"}}],\"created\":1762688262,\"id\":\"chatcmpl-TGowQQupfyXM8Lepxdl07oStPvvn1NUJ\",\"model\":\"\",\"object
\":\"chat.completion.chunk\",\"usage\":{\"completion_tokens\":1,\"prompt_tokens\":5,\"total_tokens\":6}}\n\n"
VERB [                    send] send new result | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [                    send] queue_results.push_back | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [           process_token] next token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 token=81918 token_text=">\")" has_next_token=true n_remain=-1 n_decoded=2 stopped_eos=false stopped_word=false stopped_limit=false stopping_word=
""
VERB [            update_slots] run slots completed | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] wait for new task | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] new task may arrive | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_new_task | tid="140114558783488" timestamp=1762688262 id_task=2
VERB [              start_loop] update_multitasks | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_update_slots | tid="140114558783488" timestamp=1762688262
VERB [            update_slots] posting NEXT_RESPONSE | tid="140114558783488" timestamp=1762688262
VERB [                    post] new task id | tid="140114558783488" timestamp=1762688262 new_id=3
VERB [            update_slots] slot decode token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 n_ctx=73728 n_past=7 n_system_tokens=0 n_cache_tokens=7 truncated=false
VERB [            update_slots] decoding batch | tid="140114558783488" timestamp=1762688262 n_tokens=1
VERB [       server_sent_event] data stream, to_send: %s | ="data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\">\\\")\"}}],\"created\":1762688262,\"id\":\"chatcmpl-TGowQQupfyXM8Lepxdl07oStPvvn1NUJ\",\"model\":\"\",\"object
\":\"chat.completion.chunk\",\"usage\":{\"completion_tokens\":2,\"prompt_tokens\":5,\"total_tokens\":7}}\n\n"
VERB [                    send] send new result | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [                    send] queue_results.push_back | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [           process_token] next token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 token=81918 token_text=">\")" has_next_token=true n_remain=-1 n_decoded=3 stopped_eos=false stopped_word=false stopped_limit=false stopping_word=
""
VERB [            update_slots] run slots completed | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] wait for new task | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] new task may arrive | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_new_task | tid="140114558783488" timestamp=1762688262 id_task=3
VERB [              start_loop] update_multitasks | tid="140114558783488" timestamp=1762688262
VERB [              start_loop] callback_update_slots | tid="140114558783488" timestamp=1762688262
VERB [            update_slots] posting NEXT_RESPONSE | tid="140114558783488" timestamp=1762688262
VERB [                    post] new task id | tid="140114558783488" timestamp=1762688262 new_id=4
VERB [            update_slots] slot decode token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 n_ctx=73728 n_past=8 n_system_tokens=0 n_cache_tokens=8 truncated=false
VERB [            update_slots] decoding batch | tid="140114558783488" timestamp=1762688262 n_tokens=1
VERB [       server_sent_event] data stream, to_send: %s | ="data: {\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\">\\\")\"}}],\"created\":1762688262,\"id\":\"chatcmpl-TGowQQupfyXM8Lepxdl07oStPvvn1NUJ\",\"model\":\"\",\"object
\":\"chat.completion.chunk\",\"usage\":{\"completion_tokens\":3,\"prompt_tokens\":5,\"total_tokens\":8}}\n\n"
VERB [                    send] send new result | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [                    send] queue_results.push_back | tid="140114558783488" timestamp=1762688262 id_task=0
VERB [           process_token] next token | tid="140114558783488" timestamp=1762688262 id_slot=0 id_task=0 token=81918 token_text=">\")" has_next_token=true n_remain=-1 n_decoded=4 stopped_eos=false stopped_word=false stopped_limit=false stopping_word=
""
```
</details>

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-09** at **11:42:39**

Does it work if you add -cuda fusion=0 ?

Try replacing the tensor overrides with --cpu-moe 

Try removing -ctk q8_0

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **11:47:44**

@ikawrakow 

okay one sec.  just to make sure I wasn't hallucinating before.

```
git reset --hard f76e98536f9377b29e2058cf83e0aa0450b631c8
```

```
 /opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server  --version
version: 3929 (f76e9853)
built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu
```

NO ISSUES!

```
/opt/ik_llama.cpp# mods -m glm hello

  1.  Analyze the User's Input: The user said "hello".

  2. Identify the Nature of the Input:
    â€¢ It's a greeting.
    â€¢ It's very simple, direct, and common.
    â€¢ It's an opening to a conversation.
  3. Determine the Goal of the Response:
    â€¢ Acknowledge the greeting.
    â€¢ Be friendly and welcoming.
    â€¢ Encourage further interaction.
    â€¢ Demonstrate my capabilities as a helpful AI assistant without being
    overly verbose or robotic at this initial stage.
```

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **11:53:49**

@ikawrakow 

> Does it work if you add -cuda fusion=0 ?

So I recompiled it with -DGGML_CUDA_FUSION=0.

```
 /opt/ik_llama.cpp/ik_llama.cpp/build/bin/llama-server  --version
version: 3967 (9207a48a)
built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu
```

Result:
```
 mods -m glm hello


  â”‚ ")>")>")>")>")>")>")>")>")>")>")>")>")
```

> Try replacing the tensor overrides with --cpu-moe
> Try removing -ctk q8_0

Tried that also.  No luck!

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-09** at **12:09:28**

So, nothing works after f76e98536f9377b29e2058cf83e0aa0450b631c8 ?

Can you do `git bisect` to isolate where things got broken?

But I'm also worried about you using saved cache (which could be broken from a non-working version).

---

ðŸ‘¤ **magikRUKKOLA** commented on **2025-11-09** at **12:32:19**

@ikawrakow 

> So, nothing works after [f76e985](https://github.com/ikawrakow/ik_llama.cpp/commit/f76e98536f9377b29e2058cf83e0aa0450b631c8) ?

Somehow yes.  Yesterday I thought that the latest master work[s], but no -- that was a mistake on my part.

> Can you do git bisect to isolate where things got broken?

I am not familiar with git, but I can try.

> But I'm also worried about you using saved cache (which could be broken from a non-working version).

But in order to save or restore the slot one have to use RPC commands.  I haven't done it in any of the cases I've tested.  I can remove the cache options whatsoever during the test.

---

ðŸ‘¤ **ikawrakow** commented on **2025-11-09** at **12:59:40**

> I am not familiar with git, but I can try.

Do git log >log.out. Pick a commit in the middle between latest main and the working commit. `git checkout this_commit`. Build, test. If it works, you have narrowed the issue between latest main and this commit. If it doesn't, you have narrowed it down between that commit and f76e98536f9377b29e2058cf83e0aa0450b631c8. Pick the middle between the working and not working commit, etc. There are only 40 commits between f76e98536f9377b29e2058cf83e0aa0450b631c8 and latest main, so you will do at most 6 steps to find the breaking commit.